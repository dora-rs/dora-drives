<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>dora-drives</title>
                <meta name="robots" content="noindex" />
                

        <!-- Custom HTML head -->
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

                <link rel="icon" href="favicon.svg">
                        <link rel="shortcut icon" href="favicon.png">
                <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
                <link rel="stylesheet" href="css/print.css" media="print">
        
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
                <link rel="stylesheet" href="fonts/fonts.css">
        
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
            </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="introduction.html">Introduction</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">User Guide</li><li class="chapter-item expanded "><a href="installation.html"><strong aria-hidden="true">1.</strong> installation</a></li><li class="chapter-item expanded affix "><li class="part-title">Tutorials</li><li class="chapter-item expanded "><a href="webcam_plot.html"><strong aria-hidden="true">2.</strong> Streaming a video</a></li><li class="chapter-item expanded "><a href="yolov5.html"><strong aria-hidden="true">3.</strong> Yolov5</a></li><li class="chapter-item expanded "><a href="webcam_full_nodes.html"><strong aria-hidden="true">4.</strong> Full perception</a></li><li class="chapter-item expanded "><a href="carla.html"><strong aria-hidden="true">5.</strong> Carla simulator</a></li><li class="chapter-item expanded "><a href="obstacle_location.html"><strong aria-hidden="true">6.</strong> Obstacle location</a></li><li class="chapter-item expanded "><a href="planning.html"><strong aria-hidden="true">7.</strong> Planning</a></li><li class="chapter-item expanded "><a href="control.html"><strong aria-hidden="true">8.</strong> Control</a></li><li class="chapter-item expanded "><a href="leaderboard.html"><strong aria-hidden="true">9.</strong> Leaderboard</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">Operators and Nodes</li><li class="chapter-item expanded "><a href="carla_source_node.html"><strong aria-hidden="true">10.</strong> Carla Source Node</a></li><li class="chapter-item expanded "><a href="perfect_detection_operator.html"><strong aria-hidden="true">11.</strong> Perfect detection operator</a></li><li class="chapter-item expanded "><a href="yolov5_operator.html"><strong aria-hidden="true">12.</strong> Yolov5 operator</a></li><li class="chapter-item expanded "><a href="obstacle_location_operator.html"><strong aria-hidden="true">13.</strong> Obstacle location operator</a></li><li class="chapter-item expanded "><a href="hybrid_astar_operator.html"><strong aria-hidden="true">14.</strong> Hybrid A-Star operator</a></li><li class="chapter-item expanded "><a href="pid_control_operator.html"><strong aria-hidden="true">15.</strong> PID Control operator</a></li><li class="chapter-item expanded "><a href="control_operator.html"><strong aria-hidden="true">16.</strong> Control operator</a></li><li class="chapter-item expanded "><a href="plot_operator.html"><strong aria-hidden="true">17.</strong> Plot operator</a></li><li class="chapter-item expanded affix "><li class="part-title">Reference Guide</li><li class="chapter-item expanded "><a href="data-format.html"><strong aria-hidden="true">18.</strong> data format</a></li></ol>            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                                                <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                                            </div>

                    <h1 class="menu-title">dora-drives</h1>

                    <div class="right-buttons">
                                                <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                                                                        
                    </div>
                </div>

                                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <p align="center">
    <img src="./logo.svg" width="400">
</p>
<hr />
<p><code>dora-drives</code> is a set of operators you can use with <code>dora</code> to create an autonomous driving vehicle.</p>
<p>You can test the operators on real webcam or within <a href="https://carla.org/">Carla</a>.</p>
<p>This project is in early development, and many features have yet to be implemented with breaking changes. Please don't take for granted the current design.</p>
<h2 id="documentation"><a class="header" href="#documentation">Documentation</a></h2>
<p>The documentation can be found here: <a href="https://dora-rs.github.io/dora-drives">dora-rs.github.io/dora-drives</a></p>
<p>You will be able to get started using the <a href="https://dora-rs.github.io/dora-drives/installation.html">installation section</a>.</p>
<h2 id="operators"><a class="header" href="#operators">Operators:</a></h2>
<h3 id="a-hrefhttpspaperswithcodecomtaskpoint-cloud-registrationlatestpoint-cloud-registrationa"><a class="header" href="#a-hrefhttpspaperswithcodecomtaskpoint-cloud-registrationlatestpoint-cloud-registrationa"><a href="https://paperswithcode.com/task/point-cloud-registration/latest">Point cloud registration</a></a></h3>
<ul>
<li><a href="https://github.com/XiaoshuiHuang/IMFNet">IMFNet</a> </li>
</ul>
<p><a href="https://paperswithcode.com/sota/point-cloud-registration-on-3dmatch-benchmark?p=imfnet-interpretable-multimodal-fusion-for"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/imfnet-interpretable-multimodal-fusion-for/point-cloud-registration-on-3dmatch-benchmark" alt="PWC" /></a></p>
<p><img src=https://user-images.githubusercontent.com/22787340/192553644-1d8466be-c1e1-492d-9bc2-3546a1a3ea55.png width="300"><img src=https://user-images.githubusercontent.com/22787340/192553631-d6379df5-a1a8-49b9-a5c1-7bc7b9f1ed52.png width="300"></p>
<h3 id="a-hrefhttpspaperswithcodecomtaskobject-detectionobject-dectectiona"><a class="header" href="#a-hrefhttpspaperswithcodecomtaskobject-detectionobject-dectectiona"><a href="https://paperswithcode.com/task/object-detection">Object dectection</a></a></h3>
<ul>
<li><a href="https://github.com/ultralytics/yolov5">yolov5</a> </li>
</ul>
<p><a href="https://paperswithcode.com/sota/object-detection-on-coco?p=path-aggregation-network-for-instance"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/path-aggregation-network-for-instance/object-detection-on-coco" alt="PWC" /></a></p>
<img src=https://user-images.githubusercontent.com/22787340/187723794-3623bee2-91d6-436a-a5d7-d2e363483c76.gif width="600">
<ul>
<li>Perfect detection on Carla Simulator</li>
</ul>
<h3 id="a-hrefhttpspaperswithcodecomtasktraffic-sign-recognitiontraffic-sign-recognitiona"><a class="header" href="#a-hrefhttpspaperswithcodecomtasktraffic-sign-recognitiontraffic-sign-recognitiona"><a href="https://paperswithcode.com/task/traffic-sign-recognition">Traffic sign recognition</a></a></h3>
<ul>
<li><a href="https://github.com/haixuanTao/yolov7">Custom trained yolov7 on tt100k</a></li>
</ul>
<h3 id="a-hrefhttpspaperswithcodecomtasklane-detectionlane-detectiona"><a class="header" href="#a-hrefhttpspaperswithcodecomtasklane-detectionlane-detectiona"><a href="https://paperswithcode.com/task/lane-detection">Lane detection</a></a></h3>
<ul>
<li><a href="https://github.com/hustvl/YOLOP">yolop</a> </li>
</ul>
<p><a href="https://paperswithcode.com/sota/lane-detection-on-bdd100k?p=hybridnets-end-to-end-perception-network-1"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/hybridnets-end-to-end-perception-network-1/lane-detection-on-bdd100k" alt="PWC" /></a> </p>
<img src=https://user-images.githubusercontent.com/22787340/187723841-7f3ba560-dbbe-4d43-886a-fb3b0be9247a.gif width="600">
<h3 id="a-hrefhttpspaperswithcodecomtaskdrivable-area-detectiondrivable-area-detectiona"><a class="header" href="#a-hrefhttpspaperswithcodecomtaskdrivable-area-detectiondrivable-area-detectiona"><a href="https://paperswithcode.com/task/drivable-area-detection">Drivable Area detection</a></a></h3>
<ul>
<li><a href="https://github.com/hustvl/YOLOP">yolop</a> </li>
</ul>
<p><a href="https://paperswithcode.com/sota/drivable-area-detection-on-bdd100k?p=hybridnets-end-to-end-perception-network-1"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/hybridnets-end-to-end-perception-network-1/drivable-area-detection-on-bdd100k" alt="PWC" /></a> </p>
<h3 id="a-hrefhttpspaperswithcodecomtaskmulti-object-trackingmultiple-object-trackingmota"><a class="header" href="#a-hrefhttpspaperswithcodecomtaskmulti-object-trackingmultiple-object-trackingmota"><a href="https://paperswithcode.com/task/multi-object-tracking">Multiple Object tracking(MOT)</a></a></h3>
<h4 id="a-hrefhttpsgithubcomhaixuantaoyolov5_strongsort_packagestrong-sorta"><a class="header" href="#a-hrefhttpsgithubcomhaixuantaoyolov5_strongsort_packagestrong-sorta"><a href="https://github.com/haixuanTao/yolov5_strongsort_package">strong sort</a></a></h4>
<p><a href="https://paperswithcode.com/sota/multi-object-tracking-on-mot20-1?p=strongsort-make-deepsort-great-again"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/strongsort-make-deepsort-great-again/multi-object-tracking-on-mot20-1" alt="PWC" /></a> </p>
<img src=https://user-images.githubusercontent.com/22787340/187723873-473cda4f-573d-4663-a5b9-a4df2611c482.gif width="600">
<h3 id="a-hrefhttpspaperswithcodecomtaskmotion-planningmotion-planninga"><a class="header" href="#a-hrefhttpspaperswithcodecomtaskmotion-planningmotion-planninga"><a href="https://paperswithcode.com/task/motion-planning">Motion Planning</a></a></h3>
<ul>
<li><a href="https://github.com/erdos-project/hybrid_astar_planner">Hybrid A-star</a></li>
</ul>
<img src=https://user-images.githubusercontent.com/22787340/192555777-1d5e4c5f-d654-4ef3-a019-387e56e46970.gif width="600">
<h3 id="path-tracking"><a class="header" href="#path-tracking">Path Tracking</a></h3>
<ul>
<li>Proportional Integral Derivative controller (PID)</li>
</ul>
<h2 id="future-operators"><a class="header" href="#future-operators">Future operators:</a></h2>
<ul>
<li>
<p><a href="https://paperswithcode.com/task/trajectory-prediction">Trajectory Prediction (pedestrian and vehicles)</a></p>
</li>
<li>
<p><a href="https://paperswithcode.com/task/pedestrian-detection">Pedestrian detection</a></p>
</li>
<li>
<p><a href="https://paperswithcode.com/task/semantic-segmentation">Semantic segmentation</a></p>
</li>
<li>
<p><a href="https://paperswithcode.com/task/depth-estimation">Depth estimation</a></p>
</li>
<li>
<p><a href="https://paperswithcode.com/task/multi-object-tracking">Multiple object tracking and segmentation(MOTS)</a></p>
</li>
</ul>
<h2 id="-license"><a class="header" href="#-license">‚öñÔ∏è LICENSE</a></h2>
<p>This project is licensed under Apache-2.0. Check out <a href="NOTICE.html">NOTICE.md</a> for more information.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installation"><a class="header" href="#installation">Installation</a></h1>
<h3 id="hardware-requirements"><a class="header" href="#hardware-requirements">Hardware requirements</a></h3>
<ul>
<li>NVIDIA GPU with CUDA</li>
</ul>
<h2 id="from-docker-hub"><a class="header" href="#from-docker-hub">From Docker Hub</a></h2>
<h3 id="environments"><a class="header" href="#environments">Environments</a></h3>
<table><thead><tr><th>Software</th><th>Version</th><th>Installation Guide</th></tr></thead><tbody>
<tr><td><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">nvidia-docker</a></td><td>20.10.18</td><td><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">Installation Guide</a></td></tr>
</tbody></table>
<p>To start the docker container:</p>
<pre><code class="language-bash">docker pull haixuantao/dora-drives
./scripts/launch.sh -s -g tutorials/carla_full.yaml
</code></pre>
<blockquote>
<p>This docker image has been built with my setup and it might not work on all machines. In case it doesn't work. Please check the following <code>From Source</code>.</p>
</blockquote>
<h2 id="building-docker-from-source"><a class="header" href="#building-docker-from-source">Building Docker From Source</a></h2>
<h3 id="environments-1"><a class="header" href="#environments-1">Environments</a></h3>
<table><thead><tr><th>Software</th><th>Version</th><th>Installation Guide</th></tr></thead><tbody>
<tr><td><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">nvidia-docker</a></td><td>20.10.18</td><td><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">nvidia-docker</a></td></tr>
<tr><td>Linux</td><td>Ubuntu 20.04.5 LTS</td><td></td></tr>
</tbody></table>
<p>For linux, run:</p>
<pre><code class="language-bash">git clone git@github.com:futurewei-tech/dora-drives.git
cd dora-drives
./scripts/launch.sh -b -s -g tutorials/carla_full.yaml
</code></pre>
<blockquote>
<p>This script has been built with my setup and you might need to install further dependencies that I have not listed, and additional configuration for cross-compiling on other OS.</p>
<p>If you're having build difficulties with CUDA. Check out :https://github.com/pytorch/extension-cpp/issues/71#issuecomment-1061880626 and make sure to have the exact same daemon.
You will need to have <code>/etc/docker/daemon.json</code> to be exactly:</p>
</blockquote>
<blockquote>
<pre><code class="language-json">{
   &quot;runtimes&quot;: {
       &quot;nvidia&quot;: {
           &quot;path&quot;: &quot;nvidia-container-runtime&quot;,
           &quot;runtimeArgs&quot;: []
       }
   },
   &quot;default-runtime&quot;: &quot;nvidia&quot;
}
</code></pre>
<p>And restart:</p>
<pre><code class="language-bash">sudo systemctl restart docker
</code></pre>
</blockquote>
<h2 id="using-dora-drives-without-docker"><a class="header" href="#using-dora-drives-without-docker">Using <code>dora-drives</code> without Docker</a></h2>
<table><thead><tr><th>Software</th><th>Version Tested</th><th>Installation Guide</th></tr></thead><tbody>
<tr><td>Linux</td><td>Ubuntu 20.04.5 LTS</td><td></td></tr>
<tr><td>Miniconda</td><td>22.11.1</td><td>Check the Dockerfile</td></tr>
<tr><td>Pytorch</td><td>1.11</td><td>Installation below</td></tr>
<tr><td>Carla</td><td>Carla Leaderboard</td><td>Installation below in <code>scripts/install.sh</code>. Version: <a href="https://carla-releases.s3.eu-west-3.amazonaws.com/Linux/Leaderboard/CARLA_Leaderboard_20.tar.gz">Leaderboard Version</a></td></tr>
<tr><td>NVIDIA Driver</td><td>515.86.01</td><td></td></tr>
<tr><td>CUDA</td><td>11.7</td><td></td></tr>
<tr><td>dora-rs</td><td>0.1.3</td><td>Installation below</td></tr>
</tbody></table>
<h3 id="environments-2"><a class="header" href="#environments-2">Environments</a></h3>
<h3 id="installation-1"><a class="header" href="#installation-1">Installation</a></h3>
<pre><code class="language-bash">export DORA_DEP_HOME=&lt;PATH TO A PARENT FOLDER&gt; # Ex: $HOME/Documents
export DORA_DEP_HOME=$HOME/Documents
export CARLA_HOME=$DORA_DEP_HOME/dependencies/CARLA_0.9.13
export PYLOT_HOME=$DORA_DEP_HOME
export PYTHONPATH=$PYTHONPATH:$DORA_DEP_HOME/dependencies:$DORA_DEP_HOME/dependencies/CARLA_0.9.13/PythonAPI/carla:$DORA_DEP_HOME/dependencies/Carsmos/simulate_py37


## Add missing linux dependencies
sudo apt-get -y update 
sudo apt-get -y install apt-utils git curl clang wget
sudo apt-get install -y cmake unzip libpng-dev libgeos-dev python3-opencv
sudo apt-get -y --fix-missing update &amp;&amp; sudo apt-get install --fix-missing -y libcudnn8 ssh libqt5core5a libeigen3-dev cmake qtbase5-dev libpng16-16 libtiff5 python3-tk libgeos-dev vim build-essential libopenblas-dev libssl-dev 

## Installing dependencies
conda create -n dora3.7 python=3.7 -y
conda activate dora3.7
conda install pytorch=1.11.0 torchvision=0.12.0 cudatoolkit=11.3 -c pytorch -y
pip install --upgrade pip
pip install -r install_requirements.txt
pip install -r requirements.txt


chmod +x ./scripts/*
./scripts/install.sh

## Installing dora

sudo wget https://github.com/dora-rs/dora/releases/download/v0.1.3/dora-v0.1.3-x86_64-Linux.zip &amp;&amp; sudo unzip dora-v0.1.3-x86_64-Linux.zip -d ~/.local/bin &amp;&amp; sudo mv ~/.local/bin/iceoryx/iox-roudi ~/.local/bin
</code></pre>
<h3 id="running-the-graph"><a class="header" href="#running-the-graph">Running the graph</a></h3>
<pre><code class="language-bash">export DORA_DEP_HOME=&lt;PATH TO A PARENT FOLDER&gt;
export CARLA_HOME=$DORA_DEP_HOME/dependencies/CARLA_0.9.13
export PYLOT_HOME=$DORA_DEP_HOME
export PYTHONPATH=$PYTHONPATH:$DORA_DEP_HOME/dependencies:$DORA_DEP_HOME/dependencies/CARLA_0.9.13/PythonAPI/carla:$DORA_DEP_HOME/dependencies/leaderboard:$DORA_DEP_HOME/dependencies/scenario_runner

## Running the simulation
chmod +x ./scripts/run_simulator.sh
./scripts/run_simulator.sh &amp;
dora up

## Spawn the dataflow
dora-coordinator --run-dataflow graphs/tutorials/carla_waypoints.yaml
</code></pre>
<h3 id="uninstalling-package"><a class="header" href="#uninstalling-package">Uninstalling package</a></h3>
<pre><code class="language-bash">conda remove --name dora3.7 --all
sudo rm -rf $DORA_DEP_HOME/dependencies
rm ~/.local/bin/dora*
    ```</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="getting-started"><a class="header" href="#getting-started">Getting started</a></h2>
<p>This first tutorial enables to stream a video stream from a webcam from scratch.</p>
<ol>
<li>Create a new dataflow</li>
</ol>
<pre><code class="language-bash">dora new abc_project --lang python
cd abc_project
</code></pre>
<p>This creates the following <code>abc_project</code> directory</p>
<pre><code class="language-bash">.
‚îú‚îÄ‚îÄ dataflow.yml
‚îú‚îÄ‚îÄ node_1
‚îÇ   ‚îî‚îÄ‚îÄ node_1.py
‚îú‚îÄ‚îÄ op_1
‚îÇ   ‚îî‚îÄ‚îÄ op_1.py
‚îî‚îÄ‚îÄ op_2
    ‚îî‚îÄ‚îÄ op_2.py
</code></pre>
<ol start="3">
<li>Start <code>dora-coordinator</code> in a separate terminal window</li>
</ol>
<pre><code class="language-bash"># New terminal window
dora up 
</code></pre>
<ol start="4">
<li>Start your dataflow</li>
</ol>
<pre><code class="language-bash"># Other window
dora start dataflow.yml
# Output: c95d118b-cded-4531-a0e4-cd85b7c3916c
</code></pre>
<p>The output is the unique ID of the dataflow instance, which can be used to control it through the <code>dora</code> CLI.</p>
<ol start="5">
<li>You will see in your <code>dora-coordinator</code> window operators receiving ticks.</li>
</ol>
<pre><code class="language-bash">Received input tick, with data: b''
Received input tick, with data: b''
Received input tick, with data: b''
...
</code></pre>
<ol start="6">
<li>Stop your dataflow</li>
</ol>
<pre><code class="language-bash">dora stop c95d118b-cded-4531-a0e4-cd85b7c3916c
</code></pre>
<p>(Pass the ID returned by <code>dora start</code> here.)</p>
<ol start="7">
<li>You can then add or modify operators or nodes. For adding nodes easily, you can use the <code>dora</code> CLI again:</li>
</ol>
<ul>
<li>Run <code>dora new --kind operator --lang Rust &lt;name&gt;</code> to create a new Rust operator named <code>&lt;name&gt;</code>.</li>
<li>Run <code>dora new --kind custom-node --lang Rust &lt;name&gt;</code> to create a new custom node named <code>&lt;name&gt;</code>.</li>
</ul>
<p>You need to add the created operators/nodes to your dataflow YAML file.</p>
<ol start="8">
<li>You can also download already implemented operators by putting links in the dataflow. This example will launch a webcam plot stream. </li>
</ol>
<pre><code class="language-yaml">communication:
  zenoh:
    prefix: /abc_project

nodes:
  - id: op_1
    operator:
      python: https://raw.githubusercontent.com/dora-rs/dora-drives/main/operators/webcam_op.py
      inputs:
        tick: dora/timer/millis/100
      outputs:
        - image
  - id: op_2
    operator:
      python: https://raw.githubusercontent.com/dora-rs/dora-drives/main/physicals/plot.py
      inputs:
        image: op_1/image 
</code></pre>
<ol start="9">
<li>Then restart the dataflow</li>
</ol>
<pre><code class="language-bash">dora start dataflow.yml
</code></pre>
<blockquote>
<p>Make sure to have a webcam and cv2 install via: <code>pip install opencv-python</code></p>
</blockquote>
<p>You should see a small webcam open up!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="making-the-video-stream-intelligent"><a class="header" href="#making-the-video-stream-intelligent">Making the video stream intelligent</a></h1>
<p>To add an operator, you will just have to specify it in the dataflow. Let's add a <code>yolov5</code> object detection operator that has already been written for us in <code>./operators/yolov5_op.py</code>.</p>
<pre><code class="language-python">import os
from typing import Callable

import cv2
import numpy as np
import torch
from dora import DoraStatus

DEVICE = os.environ.get(&quot;PYTORCH_DEVICE&quot;) or &quot;cpu&quot;


class Operator:
    &quot;&quot;&quot;
    Infering object from images
    &quot;&quot;&quot;

    def __init__(self):
        self.model = torch.hub.load(
            &quot;ultralytics/yolov5&quot;,
            &quot;yolov5n&quot;,
        )
        self.model.to(torch.device(DEVICE))
        self.model.eval()

    def on_input(
        self,
        dora_input: dict,
        send_output: Callable[[str, bytes], None],
    ) -&gt; DoraStatus:
        &quot;&quot;&quot;Handle image
        Args:
            dora_input[&quot;id&quot;](str): Id of the input declared in the yaml configuration
            dora_input[&quot;data&quot;] (bytes): Bytes message of the input
            send_output (Callable[[str, bytes]]): Function enabling sending output back to dora.
        &quot;&quot;&quot;
        if dora_input[&quot;id&quot;] == &quot;check&quot;:
            send_output(&quot;ready&quot;, b&quot;&quot;)
            return DoraStatus.CONTINUE

        else:
            frame = cv2.imdecode(
                np.frombuffer(
                    dora_input[&quot;data&quot;],
                    dtype=&quot;uint8&quot;,
                ),
                -1,
            )
            frame = frame[:, :, :3]

            results = self.model(frame)  # includes NMS
            arrays = np.array(results.xyxy[0].cpu())[
                :, [0, 2, 1, 3, 4, 5]
            ]  # xyxy -&gt; xxyy
            arrays[:, 4] *= 100
            arrays = arrays.astype(np.int32)
            arrays = arrays.tobytes()
            send_output(&quot;bbox&quot;, arrays, dora_input[&quot;metadata&quot;])
            return DoraStatus.CONTINUE
</code></pre>
<p>The operator basically load a pytorch model and run it on <code>jpeg</code> encoded images. 
It sends the bounding box as a numpy bytes array with type <code>int32</code>.</p>
<p>To use it, just add it to the graph:</p>
<pre><code class="language-yaml">communication:
  iceoryx:
    app_name_prefix: dora-iceoryx-example

nodes:
  - id: webcam
    operator:
      python: ../../operators/webcam_op.py
      inputs:
        tick: dora/timer/millis/100
      outputs:
        - image

  - id: yolov5
    operator: 
      outputs:
        - bbox
      inputs:
        image: webcam/image
      python: ../../operators/yolov5_op.py

  - id: plot
    operator:
      python: ../../operators/plot.py
      inputs:
        image: webcam/image
        obstacles_bbox: yolov5/bbox
        tick: dora/timer/millis/100
</code></pre>
<blockquote>
<p>I have replaced link with local files for clarity.</p>
</blockquote>
<p>Inputs are prefixed by the node name to be able to separate name conflicts.</p>
<p>I've added capabilities for the plot to show the bounding box found by the <code>yolov5</code> operator in <code>physicals/plot.py</code>, which is basically mangling with cv2 API.</p>
<ul>
<li>To run it with docker:</li>
</ul>
<pre><code class="language-bash">./scripts/launch.sh -b -g tutorials/webcam_yolov5.yaml
</code></pre>
<ul>
<li>To run it without docker:</li>
</ul>
<pre><code class="language-bash">dora-coordinator --run-dataflow graphs/tutorials/webcam_yolov5.yaml
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="full-perception"><a class="header" href="#full-perception">Full perception</a></h1>
<p>Let's add all the operators currently provided by <code>dora-drives</code> that works on image frame. We currently have implemented:</p>
<ul>
<li><code>yolov5</code> an object detector.</li>
<li><code>strong_sort</code> a multi-object tracker.</li>
<li><code>yolop</code> a lane and drivable area detector.</li>
<li><code>traffic_sign</code> a traffic sign detector.</li>
</ul>
<p>the graph will look as follows:</p>
<pre><code class="language-yaml">communication:
  iceoryx:
    app_name_prefix: dora-iceoryx-example

nodes:
  - id: webcam
    operator:
      python: ../../operators/webcam_op.py
      inputs:
        tick: dora/timer/millis/100
      outputs:
        - image

  - id: yolov5
    operator: 
      outputs:
        - bbox
      inputs:
        image: webcam/image
      python: ../../operators/yolov5_op.py

  - id: yolop
    operator: 
      outputs:
        - lanes
        - drivable_area
      inputs:
        image: webcam/image
      python: ../../operators/yolop_op.py

  ## Commented out as it takes a lot of GPU memory.
  #- id: traffic_sign
    #operator: 
      #outputs:
        #- bbox
      #inputs:
        #image: webcam/image
      #python: operators/traffic_sign_op.py

  - id: strong_sort
    operator: 
      outputs:
        - obstacles_id
      inputs:
        image: webcam/image
        obstacles_bbox: yolov5/bbox
      python: ../../operators/strong_sort_op.py

  - id: plot
    operator:
      python: ../../operators/plot.py
      inputs:
        image: webcam/image
        obstacles_bbox: yolov5/bbox
        traffic_sign_bbox: traffic_sign/bbox
        lanes: yolop/lanes
        drivable_area: yolop/drivable_area
        obstacles_id: strong_sort/obstacles_id
        tick: dora/timer/millis/100
</code></pre>
<p>Run it with docker:</p>
<pre><code class="language-bash">./scripts/launch.sh -g tutorials/webcam_full.yaml
</code></pre>
<ul>
<li>To run it without docker:</li>
</ul>
<pre><code class="language-bash">dora-coordinator --run-dataflow graphs/tutorials/webcam_full.yaml
</code></pre>
<p>Nice ü•≥ As you can see, the value of <code>dora</code> comes from the idea that you can compose different algorithm really quickly.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="carla-simulator"><a class="header" href="#carla-simulator">Carla simulator</a></h1>
<p>Let's try to use a car simulator to not only do perception but also control.</p>
<p>To launch the simulator all you have to do is add the <code>-s</code>flag in the <code>./script/launch.sh</code> command.</p>
<p>We can then switch from the webcam to the simulator in our graph.</p>
<pre><code class="language-yaml">communication:
  iceoryx:
    app_name_prefix: dora-iceoryx-example
    
nodes:
  - id: carla_source_node
    custom:
      inputs:
        tick: dora/timer/millis/300
      outputs:
        - position
        - depth_frame
        - segmented_frame
        - vehicle_id
        - image
        - lidar_pc
        - objective_waypoints
      source: python
      args: ../../carla/carla_source_node.py
      env: 
        SET_AUTOPILOT: true 

  - id: yolov5
    operator: 
      outputs:
        - bbox
      inputs:
        image: carla_source_node/image
      python: ../../operators/yolov5_op.py

  - id: yolop
    operator: 
      outputs:
        - lanes
        - drivable_area
      inputs:
        image: carla_source_node/image
      python: ../../operators/yolop_op.py

  ## Commented out as it takes a bit of GPU memory.
  #- id: strong_sort
    #operator: 
      #outputs:
        #- obstacles_id
      #inputs:
        #image: carla_source_node/image
        #obstacles_bbox: yolov5/bbox
      #python: ../../operators/strong_sort_op.py
  
  - id: obstacle_location_op
    operator: 
      outputs:
        - obstacles
        - global_lanes
      inputs:
        lidar_pc: carla_source_node/lidar_pc
        obstacles_bbox: yolov5/bbox
        position: carla_source_node/position
        lanes: yolop/lanes
      python: ../../operators/obstacle_location_op.py

  - id: plot
    operator:
      python: ../../operators/plot.py
      inputs:
        image: carla_source_node/image
        obstacles_bbox: yolov5/bbox
        obstacles: obstacle_location_op/obstacles
        traffic_sign_bbox: traffic_sign/bbox
        lanes: yolop/lanes
        global_lanes: obstacle_location_op/global_lanes
        drivable_area: yolop/drivable_area
        obstacles_id: strong_sort/obstacles_id
        tick: dora/timer/millis/400
        gps_waypoints: carla_gps_op/gps_waypoints
        position: carla_source_node/position
        waypoints: fot_op/waypoints
        control: pid_control_op/control
</code></pre>
<p>We can then tun it with the following command:</p>
<pre><code class="language-bash">./scripts/launch.sh -b -s -g tutorials/carla_perception.yaml
</code></pre>
<ul>
<li>To run it without docker:</li>
</ul>
<pre><code class="language-bash">## Make sure your environment variable are well set.
export DORA_DEP_HOME=&lt;PATH TO A PARENT FOLDER&gt; # Ex: $HOME/Documents
export CARLA_HOME=$DORA_DEP_HOME/dependencies/CARLA_0.9.10.1
export PYLOT_HOME=$DORA_DEP_HOME
export PYTHONPATH=$PYTHONPATH:$DORA_DEP_HOME/dependencies:$DORA_DEP_HOME/dependencies/CARLA_0.9.10.1/PythonAPI/carla/dist/carla-0.9.10-py3.7-linux-x86_64.egg:$DORA_DEP_HOME/dependencies/CARLA_0.9.10.1/PythonAPI/carla/:$DORA_DEP_HOME/dependencies/CARLA_0.9.10.1/PythonAPI/carla/agents/:$DORA_DEP_HOME/dependencies/CARLA_0.9.10.1/PythonAPI/

## First start the simulator if it is not started
./scripts/run_simulator.sh &amp;

dora-coordinator --run-dataflow graphs/tutorials/carla_perception.yaml
</code></pre>
<blockquote>
<p>I have removed the traffic sign operator to reduce GPU memory consumption.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="obstacle-location"><a class="header" href="#obstacle-location">Obstacle location</a></h1>
<p>The carla simulator gives us the possibility to work with many more sensors than just a camera feed. We can emulate an LIDAR, IMU, Depth sensor, segmentation sensor...</p>
<p>Let's use the LIDAR sensor to locate the exact position of the obstacle that has been located by <code>yolov5</code>.</p>
<p>The lidar point cloud is an array of <code>x, y, z, intensity</code> points.</p>
<blockquote>
<p>The coordinates are based on Unreal Engine coordinate system which is: </p>
<ul>
<li>z is up</li>
<li>x is forward</li>
<li>y is right</li>
</ul>
<p>More info: <a href="https://www.techarthub.com/a-practical-guide-to-unreal-engine-4s-coordinate-system/">https://www.techarthub.com/a-practical-guide-to-unreal-engine-4s-coordinate-system/</a></p>
<p>and within carla documentation: <a href="https://carla.readthedocs.io/en/latest/ref_sensors/#lidar-sensor">https://carla.readthedocs.io/en/latest/ref_sensors/#lidar-sensor</a></p>
</blockquote>
<p>To get the obstacle location, we are going to compute the angle of every points in the point cloud. We can then map the angle of each pixel of the bounding box to a real point and therefore infere its location. The code can be found here: <a href="https://github.com/dora-rs/dora-drives/blob/main/operators/obstacle_location_op.py"><code>operators/obstacle_location_op.py</code></a></p>
<p>To use the obstacle location, just add it to the graph with:</p>
<pre><code class="language-yaml">  - id: obstacle_location_op
    operator: 
      outputs:
        - obstacles
      inputs:
        lidar_pc: carla_source_node/lidar_pc
        obstacles_bbox: yolov5/bbox
        position: carla_source_node/position
      python: operators/obstacle_location_op.py
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="planning"><a class="header" href="#planning">Planning</a></h1>
<p>To make the car drive itself we first need to plan the way we want to go.</p>
<h2 id="gps"><a class="header" href="#gps">GPS</a></h2>
<p>To do this, we're going to use gps waypoints from our current location to our target location.</p>
<p>Luckily we have a <code>carla</code> gps operator. </p>
<p>To get the gps waypoint with a preset target location. All we have to do is add:</p>
<pre><code class="language-yaml">  - id: carla_gps_op
    operator:
      python: carla/carla_gps_op.py
      outputs:
        - gps_waypoints
      inputs:
        position: carla_source_node/position
        objective_waypoints: carla_source_node/objective_waypoints
</code></pre>
<p>It will compute waypoints from the input location to go to follow the objective waypoints. </p>
<p>By default, the first message is: <code>[[234, 59, 39]]</code>.</p>
<p>The waypoints are defined as a an array of <code>x, y, speed</code> as <code>float32</code> waypoints, with global coordinate.</p>
<h2 id="planner"><a class="header" href="#planner">Planner</a></h2>
<p>The GPS waypoints does not take into account obstacles. To avoid collision, we can implement a motion planner that can avoid obstacles. </p>
<p>We're going to reuse a model called <code>hybrid_astar</code> as a black box, that take as input a starting location and a goal location, as well as a list of obstacles and he will be able to solve the best waypoints on its own.</p>
<pre><code class="language-yaml">  - id: hybrid_astar_op
    operator:
      python: operators/hybrid_astar_op.py
      outputs:
        - waypoints
      inputs:
        position: carla_source_node/position
        obstacles: obstacle_location_op/obstacles
        gps_waypoints: carla_gps_op/gps_waypoints
</code></pre>
<p>To test waypoints algorithms, launch:</p>
<pre><code class="language-bash">./scripts/launch.sh -b -s -g tutorials/carla_waypoints.yaml
</code></pre>
<ul>
<li>To run it without docker:</li>
</ul>
<pre><code class="language-bash">dora-coordinator --run-dataflow graphs/tutorials/carla_waypoints.yaml
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="control"><a class="header" href="#control">Control</a></h1>
<p>Car can simplistically be controlled in general using 3 variables: <code>throttle, steering, brake</code>. We're going to focus on those 3 for the moment.</p>
<p>To send a control to a Carla car, we can use <code>carla_control_op.py</code> that takes an array of 3 variables: <code>throttle, steering, brake</code> and apply it to our car.</p>
<p>To translate out waypoints to those control, we're using a PID controller that is able to adjust the steering according to the response of the steering, and we're going to pipe this response to the CARLA API so that the car can move. The PID controller code is in <code>pid_control_op.py</code>.</p>
<p>The full graph look as follows:</p>
<pre><code class="language-yaml">communication:
  iceoryx:
    app_name_prefix: dora-iceoryx-example
    
nodes:
  - id: carla_source_node
    custom:
      inputs:
        tick: dora/timer/millis/500
      outputs:
        - position
        #- depth_frame
        #- segmented_frame
        - vehicle_id
        - image
        - lidar_pc
        - objective_waypoints
      source: python
      args: ../../carla/carla_source_node.py

  - id: carla_gps_op
    operator:
      python: ../../carla/carla_gps_op.py
      outputs:
        - gps_waypoints
      inputs:
        # opendrive: carla_source_node/opendrive
        objective_waypoints: carla_source_node/objective_waypoints
        position: carla_source_node/position

  - id: yolov5
    operator: 
      outputs:
        - bbox
      inputs:
        image: carla_source_node/image
      python: ../../operators/yolov5_op.py
    env:
     # CUDA_VISIBLE_DEVICES: &quot;&quot;
      PYTORCH_DEVICE: cuda

  #- id: yolop
    #operator: 
      #outputs:
        #- lanes
        #- drivable_area
      #inputs:
        #image: carla_source_node/image
      #python: ../../operators/yolop_op.py
    #env:
      # CUDA_VISIBLE_DEVICES: &quot;&quot;

  - id: obstacle_location_op
    operator: 
      outputs:
        - obstacles
        - global_lanes
      inputs:
        lidar_pc: carla_source_node/lidar_pc
        obstacles_bbox: yolov5/bbox
        position: carla_source_node/position
        lanes: yolop/lanes
      python: ../../operators/obstacle_location_op.py

  - id: fot_op
    operator:
      python: ../../operators/fot_op.py
      outputs:
        - waypoints
      inputs:
        position: carla_source_node/position
        obstacles: obstacle_location_op/obstacles
        gps_waypoints: carla_gps_op/gps_waypoints
        global_lanes: obstacle_location_op/global_lanes
 
  - id: pid_control_op
    operator:
      python: ../../operators/pid_control_op.py
      outputs:
        - control
      inputs:
        position: carla_source_node/position
        waypoints: fot_op/waypoints

  - id: plot
    operator:
      python: ../../operators/plot.py
      inputs:
        image: carla_source_node/image
        obstacles_bbox: yolov5/bbox
        obstacles: obstacle_location_op/obstacles
        traffic_sign_bbox: traffic_sign/bbox
        lanes: yolop/lanes
        global_lanes: obstacle_location_op/global_lanes
        drivable_area: yolop/drivable_area
        obstacles_id: strong_sort/obstacles_id
        tick: dora/timer/millis/400
        gps_waypoints: carla_gps_op/gps_waypoints
        position: carla_source_node/position
        waypoints: fot_op/waypoints
        control: pid_control_op/control
      #  lidar_pc: carla_source_node/lidar_pc

  - id: carla_control_op
    operator:
      python: ../../carla/carla_control_op.py
      outputs:
        - control_status
      inputs:
        control: pid_control_op/control
        vehicle_id: carla_source_node/vehicle_id
</code></pre>
<p>You can visualize your graph with:</p>
<pre><code class="language-bash">dora graph graphs/tutorials/carla_full.yaml --open                 
</code></pre>
<pre><code class="language-mermaid">        flowchart TB
  carla_source_node
subgraph carla_gps_op
  carla_gps_op/op[op]
end
subgraph yolov5
  yolov5/op[op]
end
subgraph obstacle_location_op
  obstacle_location_op/op[op]
end
subgraph fot_op
  fot_op/op[op]
end
subgraph pid_control_op
  pid_control_op/op[op]
end
subgraph plot
  plot/op[/op\]
end
subgraph carla_control_op
  carla_control_op/op[op]
end
subgraph ___dora___ [dora]
  subgraph ___timer_timer___ [timer]
    dora/timer/millis/400[\millis/400/]
    dora/timer/millis/500[\millis/500/]
  end
end
  dora/timer/millis/500 -- tick --&gt; carla_source_node
  carla_source_node -- objective_waypoints --&gt; carla_gps_op/op
  carla_source_node -- position --&gt; carla_gps_op/op
  carla_source_node -- image --&gt; yolov5/op
  missing&gt;missing] -- lanes --&gt; obstacle_location_op/op
  carla_source_node -- lidar_pc --&gt; obstacle_location_op/op
  yolov5/op -- bbox as obstacles_bbox --&gt; obstacle_location_op/op
  carla_source_node -- position --&gt; obstacle_location_op/op
  obstacle_location_op/op -- global_lanes --&gt; fot_op/op
  carla_gps_op/op -- gps_waypoints --&gt; fot_op/op
  obstacle_location_op/op -- obstacles --&gt; fot_op/op
  carla_source_node -- position --&gt; fot_op/op
  carla_source_node -- position --&gt; pid_control_op/op
  fot_op/op -- waypoints --&gt; pid_control_op/op
  pid_control_op/op -- control --&gt; plot/op
  missing&gt;missing] -- drivable_area --&gt; plot/op
  obstacle_location_op/op -- global_lanes --&gt; plot/op
  carla_gps_op/op -- gps_waypoints --&gt; plot/op
  carla_source_node -- image --&gt; plot/op
  missing&gt;missing] -- lanes --&gt; plot/op
  obstacle_location_op/op -- obstacles --&gt; plot/op
  yolov5/op -- bbox as obstacles_bbox --&gt; plot/op
  missing&gt;missing] -- obstacles_id --&gt; plot/op
  carla_source_node -- position --&gt; plot/op
  dora/timer/millis/400 -- tick --&gt; plot/op
  missing&gt;missing] -- traffic_sign_bbox --&gt; plot/op
  fot_op/op -- waypoints --&gt; plot/op
  pid_control_op/op -- control --&gt; carla_control_op/op
  carla_source_node -- vehicle_id --&gt; carla_control_op/op
</code></pre>
<p>To test it out:</p>
<pre><code class="language-bash">./scripts/launch.sh -b -s -g tutorials/carla_full.yaml
</code></pre>
<ul>
<li>To run it without docker:</li>
</ul>
<pre><code class="language-bash">dora-coordinator --run-dataflow graphs/tutorials/carla_full.yaml
</code></pre>
<p>üòé We now have a working autonomous car!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="leaderboard"><a class="header" href="#leaderboard">Leaderboard</a></h1>
<p>The OASIS Leaderboard enables you to test out your agent on predefined scenarios. It will generates KPI that can be used to compare agent between them. </p>
<p>You can use the OASIS leaderboard without changing your graph by simply changing the input source. The new source is <code>carla/oasis_agent</code>.</p>
<p>To use the leaderboard through docker, first build it with:</p>
<pre><code class="language-bash">docker build . -t haixuantao/dora-drives
</code></pre>
<p>or pull it from dockerhub:</p>
<pre><code class="language-bash">docker pull haixuantao/dora-drives
</code></pre>
<p>Then run it within docker with:</p>
<pre><code class="language-bash">docker run \
    --gpus all \
    --runtime=nvidia \
    --env-file variables.env \
    --env GRAPH=leaderboard/oasis_agent.yaml \
    --net=host \
    -it \
    --shm-size=2g \
    --memory=10g \
    --name dora \
    haixuantao/dora-drives
</code></pre>
<p>Or use the leaderboard natively:</p>
<pre><code class="language-bash">dora-coordinator --run-dataflow graphs/leaderboard/oasis_agent.yaml
</code></pre>
<p>To have a more fined-grained control over starting and stopping your agent,
you can use <code>start/stop</code> command from the cli:</p>
<pre><code class="language-bash"># In a dedicated terminal
dora up

# In a client terminal
dora start graphs/leaderboard/oasis_agent.yaml

# ...
dora stop
</code></pre>
<p>You can name your dataflow as such:</p>
<pre><code class="language-bash"># In a client terminal
dora start graphs/leaderboard/oasis_agent.yaml --name oasis-agent-demo

# ...
dora stop --name oasis-agent-demo
</code></pre>
<p>You will be able to find the results in the <code>graphs/leaderboard/</code> folder.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="carla-source-node"><a class="header" href="#carla-source-node">Carla Source Node</a></h1>
<p>Carla source node generates pedestrians, vehicles, as well as an <code>ego</code> vehicle with camera and sensors attached to it.</p>
<h2 id="outputs"><a class="header" href="#outputs">Outputs</a></h2>
<ul>
<li>Camera Frame from the simulator camera.</li>
<li>Depth Frame.</li>
<li>Segmented Frame in case of perfect detection.</li>
<li>position of the <code>ego</code> vehicle.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="perfect-detection-operator"><a class="header" href="#perfect-detection-operator">Perfect Detection Operator</a></h1>
<p>The perfect object detection operator use information from the simulator to generate bounding box on image frame.</p>
<p>It uses the simulator to retrieve all positions of object within the simulation.</p>
<h2 id="inputs"><a class="header" href="#inputs">Inputs</a></h2>
<ul>
<li>segmented frame for information about what is visible to the car.</li>
<li>depth frame to check if the object is not too far from the vehicle.</li>
<li>position of the car to check the distances between obstacles.</li>
</ul>
<h2 id="outputs-1"><a class="header" href="#outputs-1">Outputs</a></h2>
<ul>
<li>Bounding box with confidence and labels.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="yolov5-operator"><a class="header" href="#yolov5-operator">Yolov5 operator</a></h1>
<p><code>Yolov5</code> object detection operator generates bounding boxes on images where it detects object. </p>
<p><code>Yolov5</code> has not been finetuned on the simulation and is directly importing weight from Pytorch Hub.</p>
<h2 id="inputs-1"><a class="header" href="#inputs-1">Inputs</a></h2>
<ul>
<li>jpeg encoded image as input.</li>
</ul>
<h2 id="outputs-2"><a class="header" href="#outputs-2">Outputs</a></h2>
<ul>
<li>Bounding box coordinates as well as the confidence and class label as output.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="obstacle-location-operator"><a class="header" href="#obstacle-location-operator">Obstacle location operator</a></h1>
<p>The obstacle location operator match bounding box with depth frame to find the exact position of obstacles.</p>
<h2 id="inputs-2"><a class="header" href="#inputs-2">Inputs</a></h2>
<ul>
<li>Obstacles bounding box.</li>
</ul>
<h2 id="outputs-3"><a class="header" href="#outputs-3">Outputs</a></h2>
<ul>
<li>GPS location of obstacles.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hybrid-a-star-operator"><a class="header" href="#hybrid-a-star-operator">Hybrid A-Star operator</a></h1>
<p>The hybrid A-star operator is going to use obstacles information to compute waypoints from a starting point to destination point. It will use the cartography when there is no obstacles.</p>
<h2 id="inputs-3"><a class="header" href="#inputs-3">Inputs</a></h2>
<ul>
<li>Obstacles GPS location.</li>
<li>Position of the car.</li>
</ul>
<h2 id="outputs-4"><a class="header" href="#outputs-4">Outputs</a></h2>
<ul>
<li>Waypoints coordinate to follow.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pid-control-operator"><a class="header" href="#pid-control-operator">PID Control operator</a></h1>
<p><code>pid</code> control operator computes the command that needs to be executed to follow the <code>hybrid_astar</code> waypoints.</p>
<h2 id="inputs-4"><a class="header" href="#inputs-4">Inputs</a></h2>
<ul>
<li>waypoints coordinates to follow.</li>
</ul>
<h2 id="outputs-5"><a class="header" href="#outputs-5">Outputs</a></h2>
<ul>
<li>Command control.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="control-operator"><a class="header" href="#control-operator">Control operator</a></h1>
<p>The control operator enables to manipulate the Carla car. 
It will connect <code>dora</code> with the <code>carla</code> simulator.</p>
<h2 id="inputs-5"><a class="header" href="#inputs-5">Inputs</a></h2>
<ul>
<li>Command to execute on the <code>ego</code> vehicle.</li>
<li><code>ego</code> vehicle id.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="plot-operator"><a class="header" href="#plot-operator">Plot operator</a></h1>
<p>Plot operator is going to take almost all output from the graph and plot it on the camera frame.</p>
<h2 id="inputs-6"><a class="header" href="#inputs-6">Inputs</a></h2>
<ul>
<li>Camera Frame.</li>
<li>Waypoints.</li>
<li>Bounding Box.</li>
<li>Obstacle locations.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-format"><a class="header" href="#data-format">Data format</a></h1>
<p>All messages should be byte messages.</p>
<p>Best practice is to use C-order numpy arrays bytes.</p>
<p>they can be generated via the <code>.tobytes()</code> method from numpy arrays.</p>
<p>They can be read via <code>np.frombuffer</code>.</p>
<h2 id="currently-used-message-format"><a class="header" href="#currently-used-message-format">Currently used message format</a></h2>
<pre><code class="language-python">## position of the car (1, 7)
position = np.array([x, y, z, pitch, yaw, roll, forward_speed])

## frames 
frame = raw_data # image in bytes of uint8 encoded in jpeg.

## Obstacles without location (-1, 6)
bounding_box_2d = np.array([[min_x, max_x, min_y, max_y, confidence, label], ...])

## Obstacles with locations (-1, 5)
obstacles = np.array([[x, y, z, confidence, label], ...])

## waypoints to follow. Shape (-1, 3)
waypoints = np.array([x_array, y_array, speed_array])

## control for the car (1, 3)
control = np.array([throttle, steer, brake])
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                        
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                            </nav>

        </div>

        
        
        
                <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        
        
                <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
                <script type="text/javascript" src="mermaid.min.js"></script>
                <script type="text/javascript" src="mermaid-init.js"></script>
        
                        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
                
    </body>
</html>
