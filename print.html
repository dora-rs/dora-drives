<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>dora-drives</title>
                <meta name="robots" content="noindex" />
                

        <!-- Custom HTML head -->
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

                <link rel="icon" href="favicon.svg">
                        <link rel="shortcut icon" href="favicon.png">
                <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
                <link rel="stylesheet" href="css/print.css" media="print">
        
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
                <link rel="stylesheet" href="fonts/fonts.css">
        
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
            </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="introduction.html">Introduction</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">User Guide</li><li class="chapter-item expanded "><a href="installation.html"><strong aria-hidden="true">1.</strong> installation</a></li><li class="chapter-item expanded affix "><li class="part-title">Tutorials</li><li class="chapter-item expanded "><a href="webcam_plot.html"><strong aria-hidden="true">2.</strong> Streaming a video</a></li><li class="chapter-item expanded "><a href="yolov5.html"><strong aria-hidden="true">3.</strong> Yolov5</a></li><li class="chapter-item expanded "><a href="webcam_full_nodes.html"><strong aria-hidden="true">4.</strong> Full perception</a></li><li class="chapter-item expanded "><a href="carla.html"><strong aria-hidden="true">5.</strong> Carla simulator</a></li><li class="chapter-item expanded "><a href="obstacle_location.html"><strong aria-hidden="true">6.</strong> Obstacle location</a></li><li class="chapter-item expanded "><a href="planning.html"><strong aria-hidden="true">7.</strong> Planning</a></li><li class="chapter-item expanded "><a href="control.html"><strong aria-hidden="true">8.</strong> Control</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">Operators and Nodes</li><li class="chapter-item expanded "><a href="carla_source_node.html"><strong aria-hidden="true">9.</strong> Carla Source Node</a></li><li class="chapter-item expanded "><a href="perfect_detection_operator.html"><strong aria-hidden="true">10.</strong> Perfect detection operator</a></li><li class="chapter-item expanded "><a href="yolov5_operator.html"><strong aria-hidden="true">11.</strong> Yolov5 operator</a></li><li class="chapter-item expanded "><a href="obstacle_location_operator.html"><strong aria-hidden="true">12.</strong> Obstacle location operator</a></li><li class="chapter-item expanded "><a href="hybrid_astar_operator.html"><strong aria-hidden="true">13.</strong> Hybrid A-Star operator</a></li><li class="chapter-item expanded "><a href="pid_control_operator.html"><strong aria-hidden="true">14.</strong> PID Control operator</a></li><li class="chapter-item expanded "><a href="control_operator.html"><strong aria-hidden="true">15.</strong> Control operator</a></li><li class="chapter-item expanded "><a href="plot_operator.html"><strong aria-hidden="true">16.</strong> Plot operator</a></li><li class="chapter-item expanded affix "><li class="part-title">Reference Guide</li><li class="chapter-item expanded "><a href="data-format.html"><strong aria-hidden="true">17.</strong> data format</a></li></ol>            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                                                <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                                            </div>

                    <h1 class="menu-title">dora-drives</h1>

                    <div class="right-buttons">
                                                <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                                                                        
                    </div>
                </div>

                                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <p align="center">
    <img src="./logo.svg" width="400">
</p>
<hr />
<p><code>dora-drives</code> is a set of operators you can use with <code>dora</code> to create an autonomous driving vehicle.</p>
<p>You can test the operators on real webcam or within <a href="https://carla.org/">Carla</a>.</p>
<p>This project is in early development, and many features have yet to be implemented with breaking changes. Please don't take for granted the current design.</p>
<h2 id="documentation"><a class="header" href="#documentation">Documentation</a></h2>
<p>The documentation can be found here: <a href="https://dora-rs.github.io/dora-drives">dora-rs.github.io/dora-drives</a></p>
<p>You will be able to get started using the <a href="https://dora-rs.github.io/dora-drives/installation.html">installation section</a>.</p>
<h2 id="operators"><a class="header" href="#operators">Operators:</a></h2>
<h3 id="a-hrefhttpspaperswithcodecomtaskpoint-cloud-registrationlatestpoint-cloud-registrationa"><a class="header" href="#a-hrefhttpspaperswithcodecomtaskpoint-cloud-registrationlatestpoint-cloud-registrationa"><a href="https://paperswithcode.com/task/point-cloud-registration/latest">Point cloud registration</a></a></h3>
<ul>
<li><a href="https://github.com/XiaoshuiHuang/IMFNet">IMFNet</a> </li>
</ul>
<p><a href="https://paperswithcode.com/sota/point-cloud-registration-on-3dmatch-benchmark?p=imfnet-interpretable-multimodal-fusion-for"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/imfnet-interpretable-multimodal-fusion-for/point-cloud-registration-on-3dmatch-benchmark" alt="PWC" /></a></p>
<p><img src=https://user-images.githubusercontent.com/22787340/192553644-1d8466be-c1e1-492d-9bc2-3546a1a3ea55.png width="300"><img src=https://user-images.githubusercontent.com/22787340/192553631-d6379df5-a1a8-49b9-a5c1-7bc7b9f1ed52.png width="300"></p>
<h3 id="a-hrefhttpspaperswithcodecomtaskobject-detectionobject-dectectiona"><a class="header" href="#a-hrefhttpspaperswithcodecomtaskobject-detectionobject-dectectiona"><a href="https://paperswithcode.com/task/object-detection">Object dectection</a></a></h3>
<ul>
<li><a href="https://github.com/ultralytics/yolov5">yolov5</a> </li>
</ul>
<p><a href="https://paperswithcode.com/sota/object-detection-on-coco?p=path-aggregation-network-for-instance"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/path-aggregation-network-for-instance/object-detection-on-coco" alt="PWC" /></a></p>
<img src=https://user-images.githubusercontent.com/22787340/187723794-3623bee2-91d6-436a-a5d7-d2e363483c76.gif width="600">
<ul>
<li>Perfect detection on Carla Simulator</li>
</ul>
<h3 id="a-hrefhttpspaperswithcodecomtasktraffic-sign-recognitiontraffic-sign-recognitiona"><a class="header" href="#a-hrefhttpspaperswithcodecomtasktraffic-sign-recognitiontraffic-sign-recognitiona"><a href="https://paperswithcode.com/task/traffic-sign-recognition">Traffic sign recognition</a></a></h3>
<ul>
<li><a href="https://github.com/haixuanTao/yolov7">Custom trained yolov7 on tt100k</a></li>
</ul>
<h3 id="a-hrefhttpspaperswithcodecomtasklane-detectionlane-detectiona"><a class="header" href="#a-hrefhttpspaperswithcodecomtasklane-detectionlane-detectiona"><a href="https://paperswithcode.com/task/lane-detection">Lane detection</a></a></h3>
<ul>
<li><a href="https://github.com/hustvl/YOLOP">yolop</a> </li>
</ul>
<p><a href="https://paperswithcode.com/sota/lane-detection-on-bdd100k?p=hybridnets-end-to-end-perception-network-1"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/hybridnets-end-to-end-perception-network-1/lane-detection-on-bdd100k" alt="PWC" /></a> </p>
<img src=https://user-images.githubusercontent.com/22787340/187723841-7f3ba560-dbbe-4d43-886a-fb3b0be9247a.gif width="600">
<h3 id="a-hrefhttpspaperswithcodecomtaskdrivable-area-detectiondrivable-area-detectiona"><a class="header" href="#a-hrefhttpspaperswithcodecomtaskdrivable-area-detectiondrivable-area-detectiona"><a href="https://paperswithcode.com/task/drivable-area-detection">Drivable Area detection</a></a></h3>
<ul>
<li><a href="https://github.com/hustvl/YOLOP">yolop</a> </li>
</ul>
<p><a href="https://paperswithcode.com/sota/drivable-area-detection-on-bdd100k?p=hybridnets-end-to-end-perception-network-1"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/hybridnets-end-to-end-perception-network-1/drivable-area-detection-on-bdd100k" alt="PWC" /></a> </p>
<h3 id="a-hrefhttpspaperswithcodecomtaskmulti-object-trackingmultiple-object-trackingmota"><a class="header" href="#a-hrefhttpspaperswithcodecomtaskmulti-object-trackingmultiple-object-trackingmota"><a href="https://paperswithcode.com/task/multi-object-tracking">Multiple Object tracking(MOT)</a></a></h3>
<h4 id="a-hrefhttpsgithubcomhaixuantaoyolov5_strongsort_packagestrong-sorta"><a class="header" href="#a-hrefhttpsgithubcomhaixuantaoyolov5_strongsort_packagestrong-sorta"><a href="https://github.com/haixuanTao/yolov5_strongsort_package">strong sort</a></a></h4>
<p><a href="https://paperswithcode.com/sota/multi-object-tracking-on-mot20-1?p=strongsort-make-deepsort-great-again"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/strongsort-make-deepsort-great-again/multi-object-tracking-on-mot20-1" alt="PWC" /></a> </p>
<img src=https://user-images.githubusercontent.com/22787340/187723873-473cda4f-573d-4663-a5b9-a4df2611c482.gif width="600">
<h3 id="a-hrefhttpspaperswithcodecomtaskmotion-planningmotion-planninga"><a class="header" href="#a-hrefhttpspaperswithcodecomtaskmotion-planningmotion-planninga"><a href="https://paperswithcode.com/task/motion-planning">Motion Planning</a></a></h3>
<ul>
<li><a href="https://github.com/erdos-project/hybrid_astar_planner">Hybrid A-star</a></li>
</ul>
<img src=https://user-images.githubusercontent.com/22787340/192555777-1d5e4c5f-d654-4ef3-a019-387e56e46970.gif width="600">
<h3 id="path-tracking"><a class="header" href="#path-tracking">Path Tracking</a></h3>
<ul>
<li>Proportional Integral Derivative controller (PID)</li>
</ul>
<h2 id="future-operators"><a class="header" href="#future-operators">Future operators:</a></h2>
<ul>
<li>
<p><a href="https://paperswithcode.com/task/trajectory-prediction">Trajectory Prediction (pedestrian and vehicles)</a></p>
</li>
<li>
<p><a href="https://paperswithcode.com/task/pedestrian-detection">Pedestrian detection</a></p>
</li>
<li>
<p><a href="https://paperswithcode.com/task/semantic-segmentation">Semantic segmentation</a></p>
</li>
<li>
<p><a href="https://paperswithcode.com/task/depth-estimation">Depth estimation</a></p>
</li>
<li>
<p><a href="https://paperswithcode.com/task/multi-object-tracking">Multiple object tracking and segmentation(MOTS)</a></p>
</li>
</ul>
<h2 id="-license"><a class="header" href="#-license">‚öñÔ∏è LICENSE</a></h2>
<p>This project is licensed under Apache-2.0. Check out <a href="NOTICE.html">NOTICE.md</a> for more information.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installation"><a class="header" href="#installation">Installation</a></h1>
<h3 id="hardware-requirements"><a class="header" href="#hardware-requirements">Hardware requirements</a></h3>
<ul>
<li>NVIDIA GPU with CUDA</li>
</ul>
<h2 id="from-docker-hub"><a class="header" href="#from-docker-hub">From Docker Hub</a></h2>
<p>You will only need <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">nvidia-docker</a></p>
<p>To start with docker which is the easiest:</p>
<pre><code class="language-bash">docker pull haixuantao/dora-drives
docker run --gpus all --env-file variables.env --net=host -e DISPLAY -itd --shm-size=256m --name dora haixuantao/dora-drives /home/dora/workspace/dora-drives/scripts/run_simulator.sh
docker exec -itd dora /home/dora/workspace/dora-drives/bin/iox-roudi 
sleep 5
docker  exec -it dora /home/dora/workspace/dora-drives/bin/dora-coordinator run /home/dora/workspace/dora-drives/graphs/yolov5_dataflow.yaml
</code></pre>
<blockquote>
<p>This docker image has been built with my setup and it might not work on all machines. In case it doesn't work. Please check the following <code>From Source</code>.</p>
</blockquote>
<h2 id="from-source"><a class="header" href="#from-source">From Source</a></h2>
<p>You will need:</p>
<ul>
<li><a href="https://rustup.rs/">Rust</a></li>
<li><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">nvidia-docker</a></li>
<li>Python v3.8.10</li>
<li>maturin <code>pip install maturin</code></li>
<li>Linux</li>
</ul>
<p>For linux, run:</p>
<pre><code class="language-bash">git clone git@github.com:futurewei-tech/dora.git
git clone git@github.com:futurewei-tech/dora-drives.git
cd dora-drives
# Compile with Python 3.8 as default interpreter
./scripts/launch.sh -b 
</code></pre>
<blockquote>
<p>This script has been built with my setup and you might need to install further dependencies that I have not listed, and additional configuration for cross-compiling on other OS.</p>
<p>If you're having build difficulties with CUDA. Check out :https://github.com/pytorch/extension-cpp/issues/71#issuecomment-1061880626 and make sure to have the exact same daemon.
You will need to have <code>/etc/docker/daemon.json</code> to be exactly:</p>
</blockquote>
<blockquote>
<pre><code class="language-json">{
   &quot;runtimes&quot;: {
       &quot;nvidia&quot;: {
           &quot;path&quot;: &quot;nvidia-container-runtime&quot;,
           &quot;runtimeArgs&quot;: []
       }
   },
   &quot;default-runtime&quot;: &quot;nvidia&quot;
}
</code></pre>
<p>And restart:</p>
<pre><code class="language-bash">sudo systemctl restart docker
</code></pre>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="streaming-live-video-on-your-monitor"><a class="header" href="#streaming-live-video-on-your-monitor">Streaming live video on your monitor</a></h1>
<p>The principle of <code>dora</code> is to declare a dataflow of nodes that are linked with a publish-subscribe service. </p>
<p>The nodes can be either:</p>
<ul>
<li>Custom nodes: Nodes that uses <code>dora</code> node API which gives you a way to receive and send messages through the dataflow.</li>
<li>Runtime nodes: Nodes that are managed by a <code>dora-runtime</code>. They requires that applications are written as <code>operators</code> following a specific framework. This operators will have more features than the <code>dora</code> node API.</li>
</ul>
<h2 id="getting-started"><a class="header" href="#getting-started">Getting started</a></h2>
<p>Let's write a small webcam streaming application.</p>
<ul>
<li>First, we're going to write a webcam node, that is going to capture the webcam image each time it receive an input and send it as a jpeg into the <code>image</code> pubsub service.</li>
</ul>
<pre><code class="language-python">#!/usr/bin/env python
# -*- coding: utf-8 -*-

import time

import cv2
from dora import Node

node = Node()

video_capture = cv2.VideoCapture(0)

start = time.time()

# Run for 20 seconds
while time.time() - start &lt; 60:
    # Wait next input
    node.next()
    ret, frame = video_capture.read()
    if ret:
        node.send_output(&quot;image&quot;, cv2.imencode(&quot;.jpg&quot;, frame)[1].tobytes())

video_capture.release()
</code></pre>
<ul>
<li>Then, we're going to write a plot operator that is going to be managed by <code>dora-runtime</code>. It's going to decode the jpeg and plot it as a window.</li>
</ul>
<pre><code class="language-python">import os
from enum import Enum
from typing import Callable

import cv2
import numpy as np


class DoraStatus(Enum):
    CONTINUE = 0
    STOP = 1


class Operator:
    &quot;&quot;&quot;
    Plot image and bounding box
    &quot;&quot;&quot;

    def __init__(self):
        self.image = []

    def on_input(
        self,
        input_id: str,
        value: bytes,
        send_output: Callable[[str, bytes], None],
    ) -&gt; DoraStatus:
        &quot;&quot;&quot;
        Put image on a cv2 window.
        Args:
            input_id (str): Id of the input declared in the yaml configuration
            value (bytes): Bytes message of the input
            send_output (Callable[[str, bytes]]): Function enabling sending output back to dora.
        &quot;&quot;&quot;
        if input_id == &quot;image&quot;:
            frame = np.frombuffer(value, dtype=&quot;uint8&quot;)
            frame = cv2.imdecode(frame, -1)
            self.image = frame
            cv2.imshow(&quot;frame&quot;, self.image)
            if cv2.waitKey(1) &amp; 0xFF == ord(&quot;q&quot;):
                return DoraStatus.STOP

        return DoraStatus.CONTINUE

    def drop_operator(self):
        cv2.destroyAllWindows()
</code></pre>
<ul>
<li>To finish, we're going to declare the graph for <code>dora-coordinator</code> to know how to link those nodes together. In this exemple we're going to use <code>Iceoryx</code> as the pubsub provider:</li>
</ul>
<pre><code class="language-yaml">communication:
  iceoryx:
    app_name_prefix: dora-iceoryx-example

nodes:
  - id: webcam
    custom:
      run: physicals/webcam.py
      inputs:
        timer: dora/timer/millis/100
      outputs:
        - image

  - id: plot
    operator:
      python: physicals/simple_plot.py
      inputs:
        image: webcam/image
</code></pre>
<blockquote>
<p>dora timer input is a standard input that tick at the configured frequency.</p>
</blockquote>
<ul>
<li>We can now run this pipeline using a <code>launch</code> script that I wrote, that compile <code>dora</code> and its API, put them in a docker and run the dataflow. </li>
</ul>
<pre><code class="language-bash">./scripts/launch.sh -b -g tutorials/webcam.yaml
</code></pre>
<blockquote>
<p>You will need a webcam to run this tutorial.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="making-the-video-stream-intelligent"><a class="header" href="#making-the-video-stream-intelligent">Making the video stream intelligent</a></h1>
<p>To add a node, you will just have to specify it in the dataflow. Let's add a <code>yolov5</code> object detection operator that has already been written for us in <code>./operators/yolov5_op.py</code>.</p>
<pre><code class="language-python">from typing import Callable

import cv2
import numpy as np
import torch

from dora_utils import DoraStatus


class Operator:
    &quot;&quot;&quot;
    Infering object from images
    &quot;&quot;&quot;

    def __init__(self):
        self.model = torch.hub.load(&quot;ultralytics/yolov5&quot;, &quot;yolov5n&quot;)
        self.model.to(torch.device(&quot;cuda&quot;))
        self.model.eval()

    def on_input(
        self,
        _input_id: str,
        value: bytes,
        send_output: Callable[[str, bytes], None],
    ) -&gt; DoraStatus:
        &quot;&quot;&quot;Handle image
        Args:
            input_id (str): Id of the input declared in the yaml configuration
            value (bytes): Bytes message of the input
            send_output (Callable[[str, bytes]]): Function enabling sending output back to dora.
        &quot;&quot;&quot;

        frame = cv2.imdecode(
            np.frombuffer(
                value,
                dtype=&quot;uint8&quot;,
            ),
            -1,
        )
        frame = frame[:, :, :3]

        results = self.model(frame)  # includes NMS
        arrays = np.array(results.xyxy[0].cpu())[
            :, [0, 2, 1, 3, 4, 5]
        ]  # xyxy -&gt; xxyy
        arrays[:, 4] *= 100
        arrays = arrays.astype(np.int32)
        arrays = arrays.tobytes()
        send_output(&quot;bbox&quot;, arrays)
        return DoraStatus.CONTINUE
</code></pre>
<p>The operator basically load a pytorch model and run it on <code>jpeg</code> encoded images. 
It sends the bounding box as a numpy bytes array with type <code>int32</code>.</p>
<p>To use it, just add it to the graph:</p>
<pre><code class="language-yaml">communication:
  iceoryx:
    app_name_prefix: dora-iceoryx-example

nodes:
  - id: webcam
    custom:
      run: physicals/webcam.py
      inputs:
        timer: dora/timer/millis/100
      outputs:
        - image

  - id: yolov5
    operator: 
      outputs:
        - bbox
      inputs:
        image: webcam/image
      python: operators/yolov5_op.py

  - id: plot
    operator:
      python: physicals/plot.py
      inputs:
        image: webcam/image
        obstacles_bbox: yolov5/bbox
</code></pre>
<p>Inputs are prefixed by the node name to be able to separate name conflicts.</p>
<p>I've added capabilities for the plot to show the bounding box found by the <code>yolov5</code> operator in <code>physicals/plot.py</code>, which is basically mangling with cv2 API.</p>
<ul>
<li>To run it:</li>
</ul>
<pre><code class="language-bash">./scripts/launch.sh -b -g tutorials/webcam_yolov5.yaml
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="full-perception"><a class="header" href="#full-perception">Full perception</a></h1>
<p>Let's add all the operators currently provided by <code>dora-drives</code> that works on image frame. We currently have implemented:</p>
<ul>
<li><code>yolov5</code> an object detector.</li>
<li><code>strong_sort</code> a multi-object tracker.</li>
<li><code>yolop</code> a lane and drivable area detector.</li>
<li><code>traffic_sign</code> a traffic sign detector.</li>
</ul>
<p>the graph will look as follows:</p>
<pre><code class="language-yaml">communication:
  iceoryx:
    app_name_prefix: dora-iceoryx-example

nodes:
  - id: webcam
    custom:
      run: physicals/webcam.py
      inputs:
        timer: dora/timer/millis/100
      outputs:
        - image

  - id: yolov5
    operator: 
      outputs:
        - bbox
      inputs:
        image: webcam/image
      python: operators/yolov5_op.py

  - id: yolop
    operator: 
      outputs:
        - lanes
        - drivable_area
      inputs:
        image: webcam/image
      python: operators/yolop_op.py

  ## Commented out as it takes a lot of GPU memory.
  #- id: traffic_sign
    #operator: 
      #outputs:
        #- bbox
      #inputs:
        #image: webcam/image
      #python: operators/traffic_sign_op.py

  - id: strong_sort
    operator: 
      outputs:
        - obstacles_id
      inputs:
        image: webcam/image
        obstacles_bbox: yolov5/bbox
      python: operators/strong_sort_op.py

  - id: plot
    operator:
      python: physicals/plot.py
      inputs:
        image: webcam/image
        obstacles_bbox: yolov5/bbox
        traffic_sign_bbox: traffic_sign/bbox
        lanes: yolop/lanes
        drivable_area: yolop/drivable_area
        obstacles_id: strong_sort/obstacles_id
</code></pre>
<p>Run it with</p>
<pre><code class="language-bash">./scripts/launch.sh -g tutorials/webcam_full.yaml
</code></pre>
<p>Nice ü•≥ As you can see, the value of <code>dora</code> comes from the idea that you can compose different algorithm really quickly.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="carla-simulator"><a class="header" href="#carla-simulator">Carla simulator</a></h1>
<p>Let's try to use a car simulator to not only do perception but also control.</p>
<p>To launch the simulator all you have to do is add the <code>-s</code>flag in the <code>./script/launch.sh</code> command.</p>
<p>We can then switch from the webcam to the simulator in our graph.</p>
<pre><code class="language-yaml">communication:
  iceoryx:
    app_name_prefix: dora-iceoryx-example
    
nodes:
  - id: carla_source_node
    custom:
      outputs:
        - position
        - depth_frame
        - segmented_frame
        - vehicle_id
        - image
        - lidar_pc
      run: carla/carla_source_node.py
      env: 
        SET_AUTOPILOT: true 

  - id: yolov5
    operator: 
      outputs:
        - bbox
      inputs:
        image: carla_source_node/image
      python: operators/yolov5_op.py

  - id: yolop
    operator: 
      outputs:
        - lanes
        - drivable_area
      inputs:
        image: carla_source_node/image
      python: operators/yolop_op.py

  - id: strong_sort
    operator: 
      outputs:
        - obstacles_id
      inputs:
        image: carla_source_node/image
        obstacles_bbox: yolov5/bbox
      python: operators/strong_sort_op.py

  - id: plot
    operator:
      python: physicals/plot.py
      inputs:
        image: carla_source_node/image
        obstacles_bbox: yolov5/bbox
        obstacles_id: strong_sort/obstacles_id
        traffic_sign_bbox: traffic_sign/bbox
        lanes: yolop/lanes
        drivable_area: yolop/drivable_area
</code></pre>
<p>We can then tun it with the following command:</p>
<pre><code class="language-bash">./scripts/launch.sh -b -s -g tutorials/carla_perception.yaml
</code></pre>
<blockquote>
<p>I have removed the traffic sign operator to reduce GPU memory consumption.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="obstacle-location"><a class="header" href="#obstacle-location">Obstacle location</a></h1>
<p>The carla simulator gives us the possibility to work with many more sensors than just a camera feed. We can emulate an LIDAR, IMU, Depth sensor, segmentation sensor...</p>
<p>Let's use the LIDAR sensor to locate the exact position of the obstacle that has been located by <code>yolov5</code>.</p>
<p>The lidar point cloud is an array of <code>x, y, z, intensity</code> points.</p>
<blockquote>
<p>The coordinates are based on Unreal Engine coordinate system which is: </p>
<ul>
<li>z is up</li>
<li>x is forward</li>
<li>y is right</li>
</ul>
<p>More info: <a href="https://www.techarthub.com/a-practical-guide-to-unreal-engine-4s-coordinate-system/">https://www.techarthub.com/a-practical-guide-to-unreal-engine-4s-coordinate-system/</a></p>
<p>and within carla documentation: <a href="https://carla.readthedocs.io/en/latest/ref_sensors/#lidar-sensor">https://carla.readthedocs.io/en/latest/ref_sensors/#lidar-sensor</a></p>
</blockquote>
<p>To get the obstacle location, we are going to compute the angle of every points in the point cloud. We can then map the angle of each pixel of the bounding box to a real point and therefore infere its location. The code can be found here: <a href="https://github.com/dora-rs/dora-drives/blob/main/operators/obstacle_location_op.py"><code>operators/obstacle_location_op.py</code></a></p>
<p>To use the obstacle location, just add it to the graph with:</p>
<pre><code class="language-yaml">  - id: obstacle_location_op
    operator: 
      outputs:
        - obstacles
      inputs:
        lidar_pc: carla_source_node/lidar_pc
        obstacles_bbox: yolov5/bbox
        position: carla_source_node/position
      python: operators/obstacle_location_op.py
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="planning"><a class="header" href="#planning">Planning</a></h1>
<p>To make the car drive itself we first need to plan the way we want to go.</p>
<h2 id="gps"><a class="header" href="#gps">GPS</a></h2>
<p>To do this, we're going to use gps waypoints from our current location to our target location.</p>
<p>Luckily we have a <code>carla</code> gps operator. </p>
<p>To get the gps waypoint with a preset target location. All we have to do is add:</p>
<pre><code class="language-yaml">  - id: carla_gps_op
    operator:
      python: carla/carla_gps_op.py
      outputs:
        - gps_waypoints
      inputs:
        position: carla_source_node/position
</code></pre>
<p>It will compute waypoints from the input location to go to the preset target location. </p>
<p>The waypoints are defined as a an array of <code>x, y, speed</code> as <code>float32</code> waypoints, with global coordinate.</p>
<h2 id="planner"><a class="header" href="#planner">Planner</a></h2>
<p>The GPS waypoints does not take into account obstacles. To avoid collision, we can implement a motion planner that can avoid obstacles. </p>
<p>We're going to reuse a model called <code>hybrid_astar</code> as a black box, that take as input a starting location and a goal location, as well as a list of obstacles and he will be able to solve the best waypoints on its own.</p>
<pre><code class="language-yaml">  - id: hybrid_astar_op
    operator:
      python: operators/hybrid_astar_op.py
      outputs:
        - waypoints
      inputs:
        position: carla_source_node/position
        obstacles: obstacle_location_op/obstacles
        gps_waypoints: carla_gps_op/gps_waypoints
</code></pre>
<p>To test waypoints algorithms, launch:</p>
<pre><code class="language-bash">./scripts/launch.sh -b -s -g tutorials/carla_waypoints.yaml
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="control"><a class="header" href="#control">Control</a></h1>
<p>Car can simplistically be controlled in general using 3 variables: <code>throttle, steering, brake</code>. We're going to focus on those 3 for the moment.</p>
<p>To send a control to a Carla car, we can use <code>carla_control_op.py</code> that takes an array of 3 variables: <code>throttle, steering, brake</code> and apply it to our car.</p>
<p>To translate out waypoints to those control, we're using a PID controller that is able to adjust the steering according to the response of the steering, and we're going to pipe this response to the CARLA API so that the car can move. The PID controller code is in <code>pid_control_op.py</code>.</p>
<p>The full graph look as follows</p>
<pre><code class="language-yaml">communication:
  iceoryx:
    app_name_prefix: dora-iceoryx-example
    
nodes:
  - id: carla_source_node
    custom:
      outputs:
        - position
        - depth_frame
        - segmented_frame
        - vehicle_id
        - image
        - lidar_pc
      run: carla/carla_source_node.py

  - id: yolov5
    operator: 
      outputs:
        - bbox
      inputs:
        image: carla_source_node/image
      python: operators/yolov5_op.py
  
  - id: obstacle_location_op
    operator: 
      outputs:
        - obstacles
      inputs:
        # depth_frame: carla_source_node/depth_frame
        lidar_pc: carla_source_node/lidar_pc
        obstacles_bbox: yolov5/bbox
        position: carla_source_node/position
      python: operators/obstacle_location_op.py
      
  - id: carla_gps_op
    operator:
      python: carla/carla_gps_op.py
      outputs:
        - gps_waypoints
      inputs:
        position: carla_source_node/position

  - id: hybrid_astar_op
    operator:
      python: operators/hybrid_astar_op.py
      outputs:
        - waypoints
      inputs:
        position: carla_source_node/position
        obstacles: obstacle_location_op/obstacles
        gps_waypoints: carla_gps_op/gps_waypoints

  - id: pid_control_op
    operator:
      python: operators/pid_control_op.py
      outputs:
        - control
      inputs:
        position: carla_source_node/position
        waypoints: hybrid_astar_op/waypoints

  - id: carla_control_op
    operator:
      python: carla/carla_control_op.py
      outputs:
        - control_status
      inputs:
        control: pid_control_op/control
        vehicle_id: carla_source_node/vehicle_id

  - id: plot_op
    operator:
      description: Plot operator
      inputs: 
        waypoints: hybrid_astar_op/waypoints
        image: carla_source_node/image
        obstacles: obstacle_location_op/obstacles
        obstacles_bbox: yolov5/bbox
        position: carla_source_node/position
      python: operators/plot_op.py
</code></pre>
<pre><code class="language-mermaid">flowchart TB
  carla_source_node[\carla_source_node/]
subgraph yolov5
  yolov5/op[op]
end
subgraph obstacle_location_op
  obstacle_location_op/op[op]
end
subgraph carla_gps_op
  carla_gps_op/op[op]
end
subgraph hybrid_astar_op
  hybrid_astar_op/op[op]
end
subgraph pid_control_op
  pid_control_op/op[op]
end
subgraph carla_control_op
  carla_control_op/op[op]
end
subgraph plot_op
  plot_op/op[/op\]
end
  carla_source_node -- image --&gt; yolov5/op
  carla_source_node -- lidar_pc --&gt; obstacle_location_op/op
  yolov5/op -- bbox as obstacles_bbox --&gt; obstacle_location_op/op
  carla_source_node -- position --&gt; obstacle_location_op/op
  carla_source_node -- position --&gt; carla_gps_op/op
  carla_gps_op/op -- gps_waypoints --&gt; hybrid_astar_op/op
  obstacle_location_op/op -- obstacles --&gt; hybrid_astar_op/op
  carla_source_node -- position --&gt; hybrid_astar_op/op
  carla_source_node -- position --&gt; pid_control_op/op
  hybrid_astar_op/op -- waypoints --&gt; pid_control_op/op
  pid_control_op/op -- control --&gt; carla_control_op/op
  carla_source_node -- vehicle_id --&gt; carla_control_op/op
  carla_source_node -- image --&gt; plot_op/op
  obstacle_location_op/op -- obstacles --&gt; plot_op/op
  yolov5/op -- bbox as obstacles_bbox --&gt; plot_op/op
  carla_source_node -- position --&gt; plot_op/op
  hybrid_astar_op/op -- waypoints --&gt; plot_op/op
</code></pre>
<p>To test it out:</p>
<pre><code class="language-bash">./scripts/launch.sh -b -s -g tutorials/carla_full.yaml
</code></pre>
<p>üòé We now have a working autonomous car!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="carla-source-node"><a class="header" href="#carla-source-node">Carla Source Node</a></h1>
<p>Carla source node generates pedestrians, vehicles, as well as an <code>ego</code> vehicle with camera and sensors attached to it.</p>
<h2 id="outputs"><a class="header" href="#outputs">Outputs</a></h2>
<ul>
<li>Camera Frame from the simulator camera.</li>
<li>Depth Frame.</li>
<li>Segmented Frame in case of perfect detection.</li>
<li>position of the <code>ego</code> vehicle.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="perfect-detection-operator"><a class="header" href="#perfect-detection-operator">Perfect Detection Operator</a></h1>
<p>The perfect object detection operator use information from the simulator to generate bounding box on image frame.</p>
<p>It uses the simulator to retrieve all positions of object within the simulation.</p>
<h2 id="inputs"><a class="header" href="#inputs">Inputs</a></h2>
<ul>
<li>segmented frame for information about what is visible to the car.</li>
<li>depth frame to check if the object is not too far from the vehicle.</li>
<li>position of the car to check the distances between obstacles.</li>
</ul>
<h2 id="outputs-1"><a class="header" href="#outputs-1">Outputs</a></h2>
<ul>
<li>Bounding box with confidence and labels.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="yolov5-operator"><a class="header" href="#yolov5-operator">Yolov5 operator</a></h1>
<p><code>Yolov5</code> object detection operator generates bounding boxes on images where it detects object. </p>
<p><code>Yolov5</code> has not been finetuned on the simulation and is directly importing weight from Pytorch Hub.</p>
<h2 id="inputs-1"><a class="header" href="#inputs-1">Inputs</a></h2>
<ul>
<li>jpeg encoded image as input.</li>
</ul>
<h2 id="outputs-2"><a class="header" href="#outputs-2">Outputs</a></h2>
<ul>
<li>Bounding box coordinates as well as the confidence and class label as output.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="obstacle-location-operator"><a class="header" href="#obstacle-location-operator">Obstacle location operator</a></h1>
<p>The obstacle location operator match bounding box with depth frame to find the exact position of obstacles.</p>
<h2 id="inputs-2"><a class="header" href="#inputs-2">Inputs</a></h2>
<ul>
<li>Obstacles bounding box.</li>
</ul>
<h2 id="outputs-3"><a class="header" href="#outputs-3">Outputs</a></h2>
<ul>
<li>GPS location of obstacles.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hybrid-a-star-operator"><a class="header" href="#hybrid-a-star-operator">Hybrid A-Star operator</a></h1>
<p>The hybrid A-star operator is going to use obstacles information to compute waypoints from a starting point to destination point. It will use the cartography when there is no obstacles.</p>
<h2 id="inputs-3"><a class="header" href="#inputs-3">Inputs</a></h2>
<ul>
<li>Obstacles GPS location.</li>
<li>Position of the car.</li>
</ul>
<h2 id="outputs-4"><a class="header" href="#outputs-4">Outputs</a></h2>
<ul>
<li>Waypoints coordinate to follow.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pid-control-operator"><a class="header" href="#pid-control-operator">PID Control operator</a></h1>
<p><code>pid</code> control operator computes the command that needs to be executed to follow the <code>hybrid_astar</code> waypoints.</p>
<h2 id="inputs-4"><a class="header" href="#inputs-4">Inputs</a></h2>
<ul>
<li>waypoints coordinates to follow.</li>
</ul>
<h2 id="outputs-5"><a class="header" href="#outputs-5">Outputs</a></h2>
<ul>
<li>Command control.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="control-operator"><a class="header" href="#control-operator">Control operator</a></h1>
<p>The control operator enables to manipulate the Carla car. 
It will connect <code>dora</code> with the <code>carla</code> simulator.</p>
<h2 id="inputs-5"><a class="header" href="#inputs-5">Inputs</a></h2>
<ul>
<li>Command to execute on the <code>ego</code> vehicle.</li>
<li><code>ego</code> vehicle id.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="plot-operator"><a class="header" href="#plot-operator">Plot operator</a></h1>
<p>Plot operator is going to take almost all output from the graph and plot it on the camera frame.</p>
<h2 id="inputs-6"><a class="header" href="#inputs-6">Inputs</a></h2>
<ul>
<li>Camera Frame.</li>
<li>Waypoints.</li>
<li>Bounding Box.</li>
<li>Obstacle locations.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-format"><a class="header" href="#data-format">Data format</a></h1>
<p>All messages should be byte messages.</p>
<p>Best practice is to use C-order numpy arrays bytes.</p>
<p>they can be generated via the <code>.tobytes()</code> method from numpy arrays.</p>
<p>They can be read via <code>np.frombuffer</code>.</p>
<h2 id="currently-used-message-format"><a class="header" href="#currently-used-message-format">Currently used message format</a></h2>
<pre><code class="language-python">## position of the car (1, 7)
position = np.array([x, y, z, pitch, yaw, roll, forward_speed])

## frames 
frame = raw_data # image in bytes of uint8 encoded in jpeg.

## Obstacles without location (-1, 6)
bounding_box_2d = np.array([[min_x, max_x, min_y, max_y, confidence, label], ...])

## Obstacles with locations (-1, 5)
obstacles = np.array([[x, y, z, confidence, label], ...])

## waypoints to follow. Shape (-1, 3)
waypoints = np.array([x_array, y_array, speed_array])

## control for the car (1, 3)
control = np.array([throttle, steer, brake])
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                        
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                            </nav>

        </div>

        
        
        
                <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        
        
                <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
                <script type="text/javascript" src="mermaid.min.js"></script>
                <script type="text/javascript" src="mermaid-init.js"></script>
        
                        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
                
    </body>
</html>
