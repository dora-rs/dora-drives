<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>dora-drives</title>
                <meta name="robots" content="noindex" />
                

        <!-- Custom HTML head -->
        

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

                <link rel="icon" href="favicon.svg">
                        <link rel="shortcut icon" href="favicon.png">
                <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
                <link rel="stylesheet" href="css/print.css" media="print">
        
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
                <link rel="stylesheet" href="fonts/fonts.css">
        
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
            </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="introduction.html">Introduction</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">User Guide</li><li class="chapter-item expanded "><a href="installation.html"><strong aria-hidden="true">1.</strong> installation</a></li><li class="chapter-item expanded affix "><li class="part-title">Tutorials Webcam</li><li class="chapter-item expanded "><a href="webcam_plot.html"><strong aria-hidden="true">2.</strong> Streaming a video</a></li><li class="chapter-item expanded "><a href="yolov5.html"><strong aria-hidden="true">3.</strong> Yolov5</a></li><li class="chapter-item expanded "><a href="webcam_full_nodes.html"><strong aria-hidden="true">4.</strong> Full perception</a></li><li class="chapter-item expanded affix "><li class="part-title">Tutorials Carla</li><li class="chapter-item expanded "><a href="carla.html"><strong aria-hidden="true">5.</strong> Carla simulator</a></li><li class="chapter-item expanded "><a href="obstacle_location.html"><strong aria-hidden="true">6.</strong> Obstacle location</a></li><li class="chapter-item expanded "><a href="planning.html"><strong aria-hidden="true">7.</strong> Planning</a></li><li class="chapter-item expanded "><a href="control.html"><strong aria-hidden="true">8.</strong> Control</a></li><li class="spacer"></li><li class="chapter-item expanded affix "><li class="part-title">Operators and Nodes</li><li class="chapter-item expanded "><a href="gps_operator.html"><strong aria-hidden="true">9.</strong> GPS operator</a></li><li class="chapter-item expanded "><a href="yolov5_operator.html"><strong aria-hidden="true">10.</strong> Yolov5 operator</a></li><li class="chapter-item expanded "><a href="obstacle_location_operator.html"><strong aria-hidden="true">11.</strong> Obstacle location operator</a></li><li class="chapter-item expanded "><a href="webcam_midas_dpt.html"><strong aria-hidden="true">12.</strong> MiDaS operator</a></li><li class="chapter-item expanded "><a href="fot_operator.html"><strong aria-hidden="true">13.</strong> FOT operator</a></li><li class="chapter-item expanded "><a href="pid_control_operator.html"><strong aria-hidden="true">14.</strong> PID Control operator</a></li><li class="chapter-item expanded affix "><li class="part-title">Reference Guide</li><li class="chapter-item expanded "><a href="overview.html"><strong aria-hidden="true">15.</strong> Overview</a></li><li class="chapter-item expanded "><a href="data-format.html"><strong aria-hidden="true">16.</strong> data format</a></li></ol>            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                                                <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                                            </div>

                    <h1 class="menu-title">dora-drives</h1>

                    <div class="right-buttons">
                                                <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                                                                        <a href="https://github.com/dora-rs/dora-drives/tree/main/docs" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                                                
                    </div>
                </div>

                                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <p align="center">
    <img src="./logo.svg" width="400">
</p>
<hr />
<p><code>dora-drives</code> is a set of operators you can use with <code>dora</code> to create an autonomous driving vehicle.</p>
<p>You can test the operators on real webcam or within <a href="https://carla.org/">Carla</a>.</p>
<p>This project is in early development, and many features have yet to be implemented with breaking changes. Please don't take for granted the current design.</p>
<h2 id="documentation"><a class="header" href="#documentation">Documentation</a></h2>
<p>The documentation can be found here: <a href="https://dora-rs.github.io/dora-drives">dora-rs.github.io/dora-drives</a></p>
<p>You will be able to get started using the <a href="https://dora-rs.github.io/dora-drives/installation.html">installation section</a>.</p>
<h2 id="operators"><a class="header" href="#operators">Operators:</a></h2>
<h3 id="a-hrefhttpspaperswithcodecomtaskpoint-cloud-registrationlatestpoint-cloud-registrationa"><a class="header" href="#a-hrefhttpspaperswithcodecomtaskpoint-cloud-registrationlatestpoint-cloud-registrationa"><a href="https://paperswithcode.com/task/point-cloud-registration/latest">Point cloud registration</a></a></h3>
<ul>
<li><a href="https://github.com/XiaoshuiHuang/IMFNet">IMFNet</a> </li>
</ul>
<p><a href="https://paperswithcode.com/sota/point-cloud-registration-on-3dmatch-benchmark?p=imfnet-interpretable-multimodal-fusion-for"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/imfnet-interpretable-multimodal-fusion-for/point-cloud-registration-on-3dmatch-benchmark" alt="PWC" /></a></p>
<p><img src=https://user-images.githubusercontent.com/22787340/192553644-1d8466be-c1e1-492d-9bc2-3546a1a3ea55.png width="300"><img src=https://user-images.githubusercontent.com/22787340/192553631-d6379df5-a1a8-49b9-a5c1-7bc7b9f1ed52.png width="300"></p>
<h3 id="a-hrefhttpspaperswithcodecomtaskobject-detectionobject-dectectiona"><a class="header" href="#a-hrefhttpspaperswithcodecomtaskobject-detectionobject-dectectiona"><a href="https://paperswithcode.com/task/object-detection">Object dectection</a></a></h3>
<ul>
<li><a href="https://github.com/ultralytics/yolov5">yolov5</a> </li>
</ul>
<p><a href="https://paperswithcode.com/sota/object-detection-on-coco?p=path-aggregation-network-for-instance"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/path-aggregation-network-for-instance/object-detection-on-coco" alt="PWC" /></a></p>
<img src=https://user-images.githubusercontent.com/22787340/187723794-3623bee2-91d6-436a-a5d7-d2e363483c76.gif width="600">
<ul>
<li>Perfect detection on Carla Simulator</li>
</ul>
<h3 id="a-hrefhttpspaperswithcodecomtasktraffic-sign-recognitiontraffic-sign-recognitiona"><a class="header" href="#a-hrefhttpspaperswithcodecomtasktraffic-sign-recognitiontraffic-sign-recognitiona"><a href="https://paperswithcode.com/task/traffic-sign-recognition">Traffic sign recognition</a></a></h3>
<ul>
<li><a href="https://github.com/haixuanTao/yolov7">Custom trained yolov7 on tt100k</a></li>
</ul>
<h3 id="a-hrefhttpspaperswithcodecomtasklane-detectionlane-detectiona"><a class="header" href="#a-hrefhttpspaperswithcodecomtasklane-detectionlane-detectiona"><a href="https://paperswithcode.com/task/lane-detection">Lane detection</a></a></h3>
<ul>
<li><a href="https://github.com/hustvl/YOLOP">yolop</a> </li>
</ul>
<p><a href="https://paperswithcode.com/sota/lane-detection-on-bdd100k?p=hybridnets-end-to-end-perception-network-1"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/hybridnets-end-to-end-perception-network-1/lane-detection-on-bdd100k" alt="PWC" /></a> </p>
<img src=https://user-images.githubusercontent.com/22787340/187723841-7f3ba560-dbbe-4d43-886a-fb3b0be9247a.gif width="600">
<h3 id="a-hrefhttpspaperswithcodecomtaskdrivable-area-detectiondrivable-area-detectiona"><a class="header" href="#a-hrefhttpspaperswithcodecomtaskdrivable-area-detectiondrivable-area-detectiona"><a href="https://paperswithcode.com/task/drivable-area-detection">Drivable Area detection</a></a></h3>
<ul>
<li><a href="https://github.com/hustvl/YOLOP">yolop</a> </li>
</ul>
<p><a href="https://paperswithcode.com/sota/drivable-area-detection-on-bdd100k?p=hybridnets-end-to-end-perception-network-1"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/hybridnets-end-to-end-perception-network-1/drivable-area-detection-on-bdd100k" alt="PWC" /></a> </p>
<h3 id="a-hrefhttpspaperswithcodecomtaskmulti-object-trackingmultiple-object-trackingmota"><a class="header" href="#a-hrefhttpspaperswithcodecomtaskmulti-object-trackingmultiple-object-trackingmota"><a href="https://paperswithcode.com/task/multi-object-tracking">Multiple Object tracking(MOT)</a></a></h3>
<h4 id="a-hrefhttpsgithubcomhaixuantaoyolov5_strongsort_packagestrong-sorta"><a class="header" href="#a-hrefhttpsgithubcomhaixuantaoyolov5_strongsort_packagestrong-sorta"><a href="https://github.com/haixuanTao/yolov5_strongsort_package">strong sort</a></a></h4>
<p><a href="https://paperswithcode.com/sota/multi-object-tracking-on-mot20-1?p=strongsort-make-deepsort-great-again"><img src="https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/strongsort-make-deepsort-great-again/multi-object-tracking-on-mot20-1" alt="PWC" /></a> </p>
<img src=https://user-images.githubusercontent.com/22787340/187723873-473cda4f-573d-4663-a5b9-a4df2611c482.gif width="600">
<h3 id="a-hrefhttpspaperswithcodecomtaskmotion-planningmotion-planninga"><a class="header" href="#a-hrefhttpspaperswithcodecomtaskmotion-planningmotion-planninga"><a href="https://paperswithcode.com/task/motion-planning">Motion Planning</a></a></h3>
<ul>
<li><a href="https://github.com/erdos-project/hybrid_astar_planner">Hybrid A-star</a></li>
</ul>
<img src=https://user-images.githubusercontent.com/22787340/192555777-1d5e4c5f-d654-4ef3-a019-387e56e46970.gif width="600">
<h3 id="path-tracking"><a class="header" href="#path-tracking">Path Tracking</a></h3>
<ul>
<li>Proportional Integral Derivative controller (PID)</li>
</ul>
<h2 id="future-operators"><a class="header" href="#future-operators">Future operators:</a></h2>
<ul>
<li>
<p><a href="https://paperswithcode.com/task/trajectory-prediction">Trajectory Prediction (pedestrian and vehicles)</a></p>
</li>
<li>
<p><a href="https://paperswithcode.com/task/pedestrian-detection">Pedestrian detection</a></p>
</li>
<li>
<p><a href="https://paperswithcode.com/task/semantic-segmentation">Semantic segmentation</a></p>
</li>
<li>
<p><a href="https://paperswithcode.com/task/depth-estimation">Depth estimation</a></p>
</li>
<li>
<p><a href="https://paperswithcode.com/task/multi-object-tracking">Multiple object tracking and segmentation(MOTS)</a></p>
</li>
</ul>
<h2 id="-license"><a class="header" href="#-license">⚖️ LICENSE</a></h2>
<p>This project is licensed under Apache-2.0. Check out <a href="NOTICE.html">NOTICE.md</a> for more information.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installation"><a class="header" href="#installation">Installation</a></h1>
<h3 id="hardware-requirements"><a class="header" href="#hardware-requirements">Hardware requirements</a></h3>
<ul>
<li>NVIDIA GPU with CUDA</li>
</ul>
<h2 id="from-docker-hub"><a class="header" href="#from-docker-hub">From Docker Hub</a></h2>
<h3 id="environments"><a class="header" href="#environments">Environments</a></h3>
<table><thead><tr><th>Software</th><th>Version</th><th>Installation Guide</th></tr></thead><tbody>
<tr><td><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">nvidia-docker</a></td><td>20.10.18</td><td><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">Installation Guide</a></td></tr>
</tbody></table>
<p>To start the docker container:</p>
<pre><code class="language-bash">docker pull haixuantao/dora-drives
./scripts/launch.sh -s -g tutorials/carla_full.yaml
</code></pre>
<blockquote>
<p>This docker image has been built with my setup and it might not work on all machines. In case it doesn't work. Please check the following <code>From Source</code>.</p>
</blockquote>
<h2 id="building-docker-from-source"><a class="header" href="#building-docker-from-source">Building Docker From Source</a></h2>
<h3 id="environments-1"><a class="header" href="#environments-1">Environments</a></h3>
<table><thead><tr><th>Software</th><th>Version</th><th>Installation Guide</th></tr></thead><tbody>
<tr><td><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">nvidia-docker</a></td><td>20.10.18</td><td><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html">nvidia-docker</a></td></tr>
<tr><td>Linux</td><td>Ubuntu 20.04.5 LTS</td><td></td></tr>
</tbody></table>
<p>For linux, run:</p>
<pre><code class="language-bash">git clone git@github.com:dora-rs/dora-drives.git
cd dora-drives
./scripts/launch.sh -b -s -g tutorials/carla_full.yaml
</code></pre>
<blockquote>
<p>This script has been built with my setup and you might need to install further dependencies that I have not listed, and additional configuration for cross-compiling on other OS.</p>
<p>If you're having build difficulties with CUDA. Check out :https://github.com/pytorch/extension-cpp/issues/71#issuecomment-1061880626 and make sure to have the exact same daemon.
You will need to have <code>/etc/docker/daemon.json</code> to be exactly:</p>
</blockquote>
<blockquote>
<pre><code class="language-json">{
   &quot;runtimes&quot;: {
       &quot;nvidia&quot;: {
           &quot;path&quot;: &quot;nvidia-container-runtime&quot;,
           &quot;runtimeArgs&quot;: []
       }
   },
   &quot;default-runtime&quot;: &quot;nvidia&quot;
}
</code></pre>
<p>And restart:</p>
<pre><code class="language-bash">sudo systemctl restart docker
</code></pre>
</blockquote>
<h2 id="using-dora-drives-without-docker"><a class="header" href="#using-dora-drives-without-docker">Using <code>dora-drives</code> without Docker</a></h2>
<table><thead><tr><th>Software</th><th>Version Tested</th><th>Installation Guide</th></tr></thead><tbody>
<tr><td>Linux</td><td>Ubuntu 20.04.5 LTS</td><td></td></tr>
<tr><td>Miniconda</td><td>22.11.1</td><td>Check the Dockerfile</td></tr>
<tr><td>Pytorch</td><td>1.11</td><td>Installation below</td></tr>
<tr><td>Carla</td><td>Carla Leaderboard</td><td>Installation below in <code>scripts/install.sh</code>. Version: <a href="https://carla-releases.s3.eu-west-3.amazonaws.com/Linux/Leaderboard/CARLA_Leaderboard_20.tar.gz">Leaderboard Version</a></td></tr>
<tr><td>NVIDIA Driver</td><td>515.86.01</td><td></td></tr>
<tr><td>CUDA</td><td>11.7</td><td></td></tr>
<tr><td>dora-rs</td><td>0.2.0</td><td>Installation below</td></tr>
</tbody></table>
<h3 id="environments-2"><a class="header" href="#environments-2">Environments</a></h3>
<h3 id="installation-1"><a class="header" href="#installation-1">Installation</a></h3>
<pre><code class="language-bash">git clone git@github.com:dora-rs/dora-drives.git

export DORA_DEP_HOME=&lt;PATH TO A PARENT FOLDER&gt; # Ex: $HOME/Documents
export DORA_DEP_HOME=$HOME/Documents
export CARLA_HOME=$DORA_DEP_HOME/dependencies/CARLA_0.9.13
export PYLOT_HOME=$DORA_DEP_HOME
export PYTHONPATH=$PYTHONPATH:$DORA_DEP_HOME/dependencies:$DORA_DEP_HOME/dependencies/CARLA_0.9.13/PythonAPI/carla:$DORA_DEP_HOME/dependencies/Carsmos/simulate_py37


## Add missing linux dependencies
sudo apt-get -y update 
sudo apt-get -y install apt-utils git curl clang wget
sudo apt-get install -y cmake unzip libpng-dev libgeos-dev python3-opencv
sudo apt-get -y --fix-missing update &amp;&amp; sudo apt-get install --fix-missing -y libcudnn8 ssh libqt5core5a libeigen3-dev cmake qtbase5-dev libpng16-16 libtiff5 python3-tk libgeos-dev vim build-essential libopenblas-dev libssl-dev 

## Installing dependencies
conda create -n dora3.7 python=3.7 -y
conda activate dora3.7
conda install pytorch=1.11.0 torchvision=0.12.0 cudatoolkit=11.3 -c pytorch -y
pip install --upgrade pip
pip install -r install_requirements.txt
pip install -r requirements.txt


chmod +x ./scripts/*
./scripts/install.sh

## Installing Carla
cd $DORA_DEP_HOME/dependencies/
wget https://carla-releases.s3.eu-west-3.amazonaws.com/Linux/CARLA_0.9.13.tar.gz 
tar -xvf CARLA_0.9.13.tar.gz
rm CARLA_0.9.13.tar.gz

## Installing dora
sudo wget https://github.com/dora-rs/dora/releases/download/v0.2.0/dora-v0.2.0-x86_64-Linux.zip &amp;&amp; sudo unzip dora-v0.2.0-x86_64-Linux.zip -d ~/.local/bin 
</code></pre>
<h3 id="uninstalling-package"><a class="header" href="#uninstalling-package">Uninstalling package</a></h3>
<pre><code class="language-bash">conda remove --name dora3.7 --all
sudo rm -rf $DORA_DEP_HOME/dependencies
rm ~/.local/bin/dora*
    ```</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started"><a class="header" href="#getting-started">Getting started</a></h1>
<p>This first tutorial enables to stream a video stream from a webcam from scratch.</p>
<ol>
<li>Fork this project</li>
</ol>
<pre><code class="language-bash"># Go to: https://github.com/dora-rs/dora-drives/fork
#
# Then clone your fork:
git clone git@github.com:&lt;USERNAME&gt;/dora-drives.git

# Add dora as a remote source to be able to fetch updates.
git remote add dora git@github.com:dora-rs/dora-drives.git
</code></pre>
<p>You will find the following folder structure</p>
<pre><code class="language-bash">.
├── graphs # Example graph
├── operators # Exemple operators
├── carla # Carla nodes and operators that requires the CARLA API
├── ros # ROS based operators to bridge between ROS and dora
├── docs # This docs folder. You can replace the src file to keep your operator documented.
├── ... # utils folder
</code></pre>
<ol start="2">
<li>To be able to run dora, you will need to start <code>dora-coordinator</code> and <code>dora-daemon</code>:</li>
</ol>
<pre><code class="language-bash"># Start the `dora-coordinator` and `dora-daemon`. 
dora up 
</code></pre>
<ol start="3">
<li>To start a dataflow, you just need to pass a dataflow path.</li>
</ol>
<pre><code class="language-bash">conda activate dora3.7
dora start graphs/tutorials/webcam.yaml --attach --hot-reload --name webcam
</code></pre>
<blockquote>
<p><code>--attach</code>: enables you to wait for the dataflow to finish 
before returning.</p>
<p><code>--hot-reload</code>: enables you to modify Python Operator while the 
dataflow is running.</p>
<p><code>--name</code>: enables you to name a dataflow that might be simpler to use than the UUID.</p>
</blockquote>
<ol start="4">
<li>You should see a small webcam open up!</li>
</ol>
<blockquote>
<p>Make sure to have a webcam and cv2 install via: <code>pip install opencv-python</code></p>
</blockquote>
<p align="center">
    <img src="./webcam.png" width="800">
</p>
<ol start="5">
<li>
<p>To stop your dataflow, you can use <kbd>ctrl</kbd>+<kbd>c</bkbd></p>
</li>
<li>
<p>That's it! You know the basic of dora!</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="making-the-video-stream-intelligent"><a class="header" href="#making-the-video-stream-intelligent">Making the video stream intelligent</a></h1>
<p>Let's add a <code>yolov5</code> object detection operator that has already been written for us in <code>./operators/yolov5_op.py</code>. This will help us detect object as bounding boxes within the webcam stream.</p>
<pre><code class="language-python"># operators/yolov5_op.py

&quot;&quot;&quot; 
# Yolov5 operator

`Yolov5` object detection operator generates bounding boxes on images where it detects object. 

More info here: [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5)

`Yolov5` has not been finetuned on the simulation and is directly importing weight from Pytorch Hub.

In case you want to run `yolov5` without internet you can clone [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5) and download the weights you want to use from [the release page](https://github.com/ultralytics/yolov5/releases/tag/v7.0) and then specify within the yaml graph the two environments variables:
- `YOLOV5_PATH: YOUR/PATH` 
- `YOLOV5_WEIGHT_PATH: YOUR/WEIGHT/PATH`

You can also choose to allocate the model in GPU using the environment variable:
- `PYTORCH_DEVICE: cuda # or cpu`

## Inputs

- image as 1920x1080xBGR array.

## Outputs

- Bounding box coordinates as well as the confidence and class label as output.

## Graph Description

```yaml
  - id: yolov5
    operator: 
      outputs:
        - bbox
      inputs:
        image: webcam/image
      python: ../../operators/yolov5_op.py
</code></pre>
<h2 id="graph-visualisation"><a class="header" href="#graph-visualisation">Graph Visualisation</a></h2>
<pre class="mermaid">        flowchart TB
  oasis_agent
subgraph yolov5
  yolov5/op[op]
end
subgraph obstacle_location_op
  obstacle_location_op/op[op]
end
  oasis_agent -- image --&gt; yolov5/op
  yolov5/op -- bbox as obstacles_bbox --&gt; obstacle_location_op/op
</pre>
<p>&quot;&quot;&quot;</p>
<p>import os
from typing import Callable</p>
<p>import numpy as np
import pyarrow as pa
import torch
from dora import DoraStatus</p>
<p>pa.array([])  # See: https://github.com/apache/arrow/issues/34994
IMAGE_WIDTH = 1920
IMAGE_HEIGHT = 1080
DEVICE = os.environ.get(&quot;PYTORCH_DEVICE&quot;) or &quot;cpu&quot;
YOLOV5_PATH = os.environ.get(&quot;YOLOV5_PATH&quot;)
YOLOV5_WEIGHT_PATH = os.environ.get(&quot;YOLOV5_WEIGHT_PATH&quot;)</p>
<p>class Operator:
&quot;&quot;&quot;
Send <code>bbox</code> found by YOLOv5 on given <code>image</code>
&quot;&quot;&quot;</p>
<pre><code>def __init__(self):
    if YOLOV5_PATH is None:
        # With internet
        self.model = torch.hub.load(
            &quot;ultralytics/yolov5&quot;,
            &quot;yolov5n&quot;,
        )
    else:
        # Without internet
        #
        # To install:
        # cd $DORA_HOME_DEP/dependecies # Optional
        # git clone https://github.com/ultralytics/yolov5.git
        # rm yolov5/.git -rf
        # Add YOLOV5_PATH and YOLOV5_WEIGHT_PATH in your YAML graph

        self.model = torch.hub.load(
            YOLOV5_PATH,
            &quot;custom&quot;,
            path=YOLOV5_WEIGHT_PATH,
            source=&quot;local&quot;,
        )

    self.model.to(torch.device(DEVICE))
    self.model.eval()

def on_event(
    self,
    dora_event: dict,
    send_output: Callable[[str, bytes], None],
) -&gt; DoraStatus:
    if dora_event[&quot;type&quot;] == &quot;INPUT&quot;:
        return self.on_input(dora_event, send_output)
    return DoraStatus.CONTINUE

def on_input(
    self,
    dora_input: dict,
    send_output: Callable[[str, bytes], None],
) -&gt; DoraStatus:
    &quot;&quot;&quot;
    Handle image
    Args:
        dora_input[&quot;id&quot;] (str): Id of the input declared in the yaml configuration
        dora_input[&quot;value&quot;] (arrow.array (UInt8)): Bytes message of the input
        send_output (Callable[[str, bytes]]): Function enabling sending output back to dora.
    &quot;&quot;&quot;
    if dora_input[&quot;id&quot;] == &quot;image&quot;:
        frame = (
            dora_input[&quot;value&quot;]
            .to_numpy()
            .reshape((IMAGE_HEIGHT, IMAGE_WIDTH, 4))
        )
        frame = frame[:, :, :3]

        results = self.model(frame)  # includes NMS
        arrays = np.array(results.xyxy[0].cpu())[
            :, [0, 2, 1, 3, 4, 5]
        ]  # xyxy -&gt; xxyy
        arrays[:, 4] *= 100
        arrays = arrays.astype(np.int32)
        arrays = pa.array(arrays.ravel().view(np.uint8))
        send_output(&quot;bbox&quot;, arrays, dora_input[&quot;metadata&quot;])
        return DoraStatus.CONTINUE
</code></pre>
<pre><code>
&gt; Operators are composed of:
&gt;
&gt; `__init__` methods that help create the object.
&gt;
&gt; `on_event` methods that is called when an event is received. 
&gt; There is currently 4 event types:
&gt; - `STOP`: meaning that the operator was signalled to stop.
&gt; - `INPUT`: meannig that an input was received.
&gt;   - You can use `dora_event['id']`, to get the id. 
&gt;   - You can use `dora_event['data']`, to get the data. 
&gt;   - You can use `dora_event['meatadata']`, to get the metadata.
&gt; - `INPUT_CLOSED`: meannig that an input source was closed. This could be useful if the input is critical for the well behaviour of the operator.
&gt; - `ERROR`: meaning that error message was received.
&gt; - `UNKNOWN`: meaning that an unknown message was received.
&gt;
&gt; We have encapsulated `input` event in a `on_input` method but this is not required.

To add an operator within the dataflow. You need to explicit what the input and output are. You can reference node by their ids:

```yaml
# graphs/tutorials/webcam_yolov5.yaml

nodes:
  - id: webcam
    operator:
      python: ../../operators/webcam_op.py
      inputs:
        tick: dora/timer/millis/100
      outputs:
        - image
    env:
      DEVICE_INDEX: 0

  - id: yolov5
    operator: 
      outputs:
        - bbox
      inputs:
        image: webcam/image
      python: ../../operators/yolov5_op.py

  - id: plot
    operator:
      python: ../../operators/plot.py
      inputs:
        image: webcam/image
        obstacles_bbox: yolov5/bbox
</code></pre>
<p>In this case, we have connected the <code>webcam/image</code> output to the <code>image</code> input of yolov5. <code>yolov5/bbox</code> is then connected to the <code>plot/obstacles_bbox</code>.</p>
<p>Inputs are prefixed by the node name to be able to separate name conflicts.</p>
<p>To run: </p>
<pre><code class="language-bash">dora up
dora start graphs/tutorials/webcam_yolov5.yaml --attach
</code></pre>
<p align="center">
    <img src="./webcam_yolov5.png" width="800">
</p>
<blockquote>
<p>For more information on <code>yolov5</code>, go on <a href="./yolov5_operator.html">our <code>yolov5</code> detail page</a></p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="full-perception"><a class="header" href="#full-perception">Full perception</a></h1>
<p>Let's add all <code>dora-drives</code> operators that works on image frame, which are:</p>
<ul>
<li><code>yolov5</code> an object detector.</li>
<li><code>strong_sort</code> a multi-object tracker.</li>
<li><code>yolop</code> a lane and drivable area detector.</li>
<li><code>traffic_sign</code> a traffic sign detector.</li>
</ul>
<p>the graph will look as follows:</p>
<pre><code class="language-yaml"># graphs/tutorials/webcam_full.yaml

nodes:
  - id: webcam
    operator:
      python: ../../operators/webcam_op.py
      inputs:
        tick: dora/timer/millis/100
      outputs:
        - image
    env:
      DEVICE_INDEX: 0

  - id: yolov5
    operator: 
      outputs:
        - bbox
      inputs:
        image: webcam/image
      python: ../../operators/yolov5_op.py

  # - id: yolop
    # operator: 
      # outputs:
        # - lanes
        # - drivable_area
      # inputs:
        # image: webcam/image
      # python: ../../operators/yolop_op.py

  ## Commented out as it takes a lot of GPU memory.
  #- id: traffic_sign
    #operator: 
      #outputs:
        #- bbox
      #inputs:
        #image: webcam/image
      #python: operators/traffic_sign_op.py

  - id: strong_sort
    operator: 
      outputs:
        - obstacles_id
      inputs:
        image: webcam/image
        obstacles_bbox: yolov5/bbox
      python: ../../operators/strong_sort_op.py

  - id: plot
    operator:
      python: ../../operators/plot.py
      inputs:
        image: webcam/image
        obstacles_bbox: yolov5/bbox
       # traffic_sign_bbox: traffic_sign/bbox
       # lanes: yolop/lanes
       # drivable_area: yolop/drivable_area
        obstacles_id: strong_sort/obstacles_id
</code></pre>
<pre><code class="language-bash">dora start graphs/tutorials/webcam_full.yaml --attach
</code></pre>
<blockquote>
<p>I'm currently having issue running all nodes behind the GFW. You can look into it for inspiration.</p>
</blockquote>
<p>Nice 🥳 As you can see, the value of <code>dora</code> comes from the idea that you can compose different algorithm really quickly.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="carla-simulator"><a class="header" href="#carla-simulator">Carla simulator</a></h1>
<p>Let's try to use a car simulator to not only do perception but also control.</p>
<h2 id="carla-setup"><a class="header" href="#carla-setup">Carla Setup</a></h2>
<p>In the rest of the tutorial, we will accept that you have a carla simulator running at <code>localhost:2000</code> the default carla configuration.</p>
<p>To start a simulator make sure you have carla installed. You can check the <a href="./installation.html">installation page</a> if you not sure if it's installed.</p>
<p>You can also a docker version or any other method provided by <a href="https://carla.org/">Carla</a></p>
<p>In case you have used the installation script. You should be able to run a carla simulator using </p>
<pre><code class="language-bash">./scripts/run_simulator.sh
</code></pre>
<blockquote>
<p>You can define the VULKAN interface you want to use, using the env variable <code>VK_ICD_FILENAMES</code>. </p>
<p>Ex for NVIDIA: <code>export VK_ICD_FILENAMES=&quot;/usr/share/vulkan/icd.d/nvidia_icd.json&quot;</code></p>
</blockquote>
<p>If you're using the OASIS platflorm, follow the OASIS platform to start and run your dataflow.</p>
<h2 id="switching-from-the"><a class="header" href="#switching-from-the">Switching from the</a></h2>
<p>We can then switch from the webcam to the simulator in our graph.</p>
<pre><code class="language-yaml"># graphs/oasis/oasis_agent_gps.yaml

nodes:
  - id: oasis_agent
    custom:
      inputs:
        tick: dora/timer/millis/400
      outputs:
        - position
        - speed
        - image
        - objective_waypoints
        - lidar_pc
        - opendrive
      source: shell
      # With Carla_source_node
      args: python3 ../../carla/carla_source_node.py
      #
      # Or with the OASIS AGENT
      #
      # args: &gt;
        # python3 $SIMULATE --output 
        # --oasJson --criteriaConfig $CRITERIA
        # --openscenario $XOSC
        # --agent $TEAM_AGENT
        # --agentConfig $TEAM_AGENT_CONF
        # --destination $DESTINATION
  
  - id: yolov5
    operator: 
      outputs:
        - bbox
      inputs:
        image: oasis_agent/image
      python: ../../operators/yolov5_op.py
    env:
      # CUDA_VISIBLE_DEVICES: &quot;&quot;
      # PYTORCH_DEVICE: cuda # Uncomment if you want to use CUDA
      # YOLOV5_PATH: # /PATH/TO/YOLOv5 Ex: $DORA_DEP_HOME/dependencies/yolov5
      # YOLOV5_WEIGHT_PATH: : # /PATH/TO/YOLOv5 Ex: $ORA_DEP_HOME/dependencies/yolov5/yolov5n.pt

  - id: plot
    operator:
      python: ../../operators/plot.py
      inputs:
        image: oasis_agent/image
        obstacles_bbox: yolov5/bbox
        position: oasis_agent/position
</code></pre>
<p>To run:</p>
<pre><code class="language-bash">dora up
dora start graphs/oasis/oasis_agent_yolov5.yaml --attach
</code></pre>
<p>You should see a window showing the view from a camera within the carla simulator. On this camera stream, you should see object detection happenning.</p>
<p>You can <kbd>ctrl</kbd>+<kbd>c</kbd> to kill your running 
dataflow.</p>
<p align="center">
    <img src="./yolov5.png" width="800">
</p>
<blockquote>
<p>This very first step is done to show you how to connect carla simulator with dora. It does not provide any control to move the car.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="obstacle-location"><a class="header" href="#obstacle-location">Obstacle location</a></h1>
<p>The carla simulator gives us the possibility to work with many more sensors than just a camera feed. We can emulate an LIDAR, IMU, Depth sensor, Segmentation sensor...</p>
<p>Let's use the LIDAR sensor to locate the exact position of the obstacle that has been located by <code>yolov5</code>.</p>
<blockquote>
<p>Details on lidar point cloud:
The lidar point cloud is an array of <code>x, y, z, intensity</code> points.</p>
<p>The coordinates are based on Unreal Engine coordinate system which is: </p>
<ul>
<li>z is up</li>
<li>x is forward</li>
<li>y is right</li>
</ul>
<p>More info: <a href="https://www.techarthub.com/a-practical-guide-to-unreal-engine-4s-coordinate-system/">https://www.techarthub.com/a-practical-guide-to-unreal-engine-4s-coordinate-system/</a></p>
<p>and within carla documentation: <a href="https://carla.readthedocs.io/en/latest/ref_sensors/#lidar-sensor">https://carla.readthedocs.io/en/latest/ref_sensors/#lidar-sensor</a></p>
<p>You can also check velodyne reference: <a href="https://github.com/ros-drivers/velodyne/blob/master/velodyne_pcl/README.md">https://github.com/ros-drivers/velodyne/blob/master/velodyne_pcl/README.md</a></p>
</blockquote>
<p>To get the obstacle location, we are going to compute the angle of every points in the point cloud. We can then map the angle of each pixel of the bounding box to a real point and therefore infere its location. We then transform the coordinate from the relative lIDAR coordinate system into a global coordinate system by adding the current position of the LIDAR sensor. The code can be found here: <a href="https://github.com/dora-rs/dora-drives/blob/main/operators/obstacle_location_op.py"><code>operators/obstacle_location_op.py</code></a>. </p>
<p>To use the obstacle location, just add it to the graph with:</p>
<pre><code class="language-yaml"># graphs/oasis/oasis_agent_obstacle_location.yaml

nodes:
  - id: oasis_agent
    custom:
      inputs:
        tick: dora/timer/millis/400
      outputs:
        - position
        - speed
        - image
        - objective_waypoints
        - lidar_pc
        - opendrive
      source: shell
      # With Carla_source_node
      args: python3 ../../carla/carla_source_node.py
      #
      # Or with the OASIS AGENT
      #
      # args: &gt;
        # python3 $SIMULATE --output 
        # --oasJson --criteriaConfig $CRITERIA
        # --openscenario $XOSC
        # --agent $TEAM_AGENT
        # --agentConfig $TEAM_AGENT_CONF
        # --destination $DESTINATION
  
  - id: yolov5
    operator: 
      outputs:
        - bbox
      inputs:
        image: oasis_agent/image
      python: ../../operators/yolov5_op.py

  - id: obstacle_location_op
    operator: 
      outputs:
        - obstacles
      inputs:
        lidar_pc: oasis_agent/lidar_pc
        obstacles_bbox: yolov5/bbox
        position: oasis_agent/position
      python: ../../operators/obstacle_location_op.py

  - id: plot
    operator:
      python: ../../operators/plot.py
      inputs:
        image: oasis_agent/image
        obstacles_bbox: yolov5/bbox
        position: oasis_agent/position
        obstacles: obstacle_location_op/obstacles
</code></pre>
<p>To run: </p>
<pre><code class="language-bash">dora up
dora start graphs/oasis/oasis_agent_obstacle_location.yaml --attach
</code></pre>
<p>You should be able to see a dot within the bounding box representing the estimated location in global coordinate of the obstacle.</p>
<p align="center">
    <img src="./obstacle_location.png" width="800">
</p>
<blockquote>
<p>For more information on <code>obstacle_location</code>, go on <a href="./obstacle_location_operator.html">our <code>obstacle_location</code> detail page</a></p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="planning"><a class="header" href="#planning">Planning</a></h1>
<p>To make the car drive itself we first need to plan the way we want to go.</p>
<h2 id="gps"><a class="header" href="#gps">GPS</a></h2>
<p>To do this, we're going to use gps to trace the route from our current location to our target location. </p>
<p>Carla <code>GlobalRoutePlanner</code> enables us to get the route from two points given a map. We have encapsulated this function within <code>operators/carla_gps_op.py</code>.</p>
<p>The following operator will compute the route from the current <code>position</code> to the <code>objective_waypoints</code> given an <code>opendrive</code> map. </p>
<pre><code class="language-yaml">  - id: carla_gps_op
    operator:
      python: ../../carla/carla_gps_op.py
      outputs:
        - gps_waypoints
      inputs:
        opendrive: oasis_agent/opendrive
        objective_waypoints: oasis_agent/objective_waypoints
        position: oasis_agent/position
</code></pre>
<blockquote>
<p>The waypoints are defined as a an array of <code>x, y, speed</code> as <code>float32</code> waypoints, with global coordinates.</p>
</blockquote>
<blockquote>
<p>For more information on <code>gps</code>, go on <a href="./gps_operator.html">our <code>gps</code> detail page</a></p>
</blockquote>
<h2 id="planner"><a class="header" href="#planner">Planner</a></h2>
<p>The GPS waypoints does not take into account obstacles. To avoid collision, we can implement a motion planner that can avoid obstacles. </p>
<p>We're going to reuse a model called <code>fot</code> (Frenet Optimal Trajectory) as a black box, that take as input a starting location and a goal waypoints, as well as a list of obstacles and outputs the best waypoints to follow.</p>
<pre><code class="language-yaml">  - id: fot_op
    operator:
      python: operators/fot_op.py
      outputs:
        - waypoints
      inputs:
        position: oasis_agent/position
        obstacles: obstacle_location_op/obstacles
        gps_waypoints: carla_gps_op/gps_waypoints
</code></pre>
<p>To test both functionallities:</p>
<pre><code class="language-bash">dora up
dora start graphs/oasis/oasis_agent_planning.yaml --attach
</code></pre>
<p align="center">
    <img src="./planning.png" width="800">
</p>
<blockquote>
<p>For more information on <code>fot</code>, go on <a href="./fot_operator.html">our <code>fot</code> detail page</a></p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="control"><a class="header" href="#control">Control</a></h1>
<h2 id="pid-controller"><a class="header" href="#pid-controller">PID Controller</a></h2>
<p>To translate our waypoints to <code>throttle, steering and brake</code> control, we're using a Proportional Integral Derivative (PID) controller that is able to adjust the throttle, steering and breaking according to the car position and speed by comparing it to the desired waypoints. The code can be found in <code>operator/pid_control_op.py</code>.</p>
<blockquote>
<p>For more information on <code>pid</code>, go on <a href="./pid_control_operator.html">our <code>pid</code> detail page</a></p>
</blockquote>
<h2 id="control-1"><a class="header" href="#control-1">Control</a></h2>
<p>The actual command being applied to the car is controlled within the <code>oasis_agent</code>.</p>
<h2 id="fully-looped-graph"><a class="header" href="#fully-looped-graph">Fully looped graph</a></h2>
<p>We have now all our starter kit node. They will look like this:</p>
<pre><code class="language-yaml"># graphs/oasis/oasis_agent.yaml

nodes:
  - id: oasis_agent
    custom:
      inputs:
        control: pid_control_op/control
        tick: dora/timer/millis/400
      outputs:
        - position
        - speed
        - image
        - objective_waypoints
        - lidar_pc
        - opendrive
      source: shell
      # args: &gt;
        # python3 $SIMULATE --output 
        # --oasJson --criteriaConfig $CRITERIA
        # --openscenario $XOSC
        # --agent $TEAM_AGENT
        # --agentConfig $TEAM_AGENT_CONF
        # --destination $DESTINATION
      #
      # or for Carla Standalone:
      #
      args: python3 ../../carla/carla_source_node.py

  - id: carla_gps_op
    operator:
      python: ../../carla/carla_gps_op.py
      outputs:
        - gps_waypoints
      inputs:
        opendrive: oasis_agent/opendrive
        objective_waypoints: oasis_agent/objective_waypoints
        position: oasis_agent/position

  - id: yolov5
    operator: 
      outputs:
        - bbox
      inputs:
        image: oasis_agent/image
      python: ../../operators/yolov5_op.py
    env:
      # CUDA_VISIBLE_DEVICES: &quot;&quot;
      PYTORCH_DEVICE: cuda

  - id: obstacle_location_op
    operator: 
      outputs:
        - obstacles
      inputs:
        lidar_pc: oasis_agent/lidar_pc
        obstacles_bbox: yolov5/bbox
        position: oasis_agent/position
      python: ../../operators/obstacle_location_op.py

  - id: fot_op
    operator:
      python: ../../operators/fot_op.py
      outputs:
        - waypoints
      inputs:
        position: oasis_agent/position
        speed: oasis_agent/speed
        obstacles: obstacle_location_op/obstacles
        gps_waypoints: carla_gps_op/gps_waypoints
 
  - id: pid_control_op
    operator:
      python: ../../operators/pid_control_op.py
      outputs:
        - control
      inputs:
        position: oasis_agent/position
        speed: oasis_agent/speed
        waypoints: fot_op/waypoints

  - id: plot
    operator:
      python: ../../operators/plot.py
      inputs:
        image: oasis_agent/image
        obstacles_bbox: yolov5/bbox
        obstacles: obstacle_location_op/obstacles
        gps_waypoints: carla_gps_op/gps_waypoints
        position: oasis_agent/position
        waypoints: fot_op/waypoints
        control: pid_control_op/control
</code></pre>
<p>To run a running car example:</p>
<pre><code class="language-bash">dora up
dora start graphs/oasis/oasis_agent.yaml --attach
</code></pre>
<p>😎 We now have a working autonomous car!</p>
<p>You might have noticed that improvement can be done in many place.</p>
<p>In case you need inspiration, we advise you check:</p>
<ul>
<li><code>operators/yolop_op.py</code> that enables you to detect lanes. It can be passed to the obstacle location to get the 3D position of the lanes. Those 3D position of lanes can then be passed to <code>fot</code> to plan by taking into account lanes on the floor.</li>
<li><code>operators/strong_sort.py</code> that enables tracking 2D bounding box through times. This can be useul if you want to avoid moving vehicles.</li>
<li><code>opertators/traffic_sign.py</code> that is self-trained traffic light detection based on yolov7 and tt100k. THis can be useful to avoid traffic light.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gps-operator"><a class="header" href="#gps-operator">GPS operator</a></h1>
<p>The GPS Operator take a map a current position and wished waypoints
and compute the best route to go to the wished waypoint.</p>
<p>It does by using <code>carla.GlobalRoutePlanner</code>. This modules takes an
<code>opendrive</code> map as an input and can compute most efficient route between 
two points.</p>
<ul>
<li>The caching of result such that we don't compute it every single time. The computation of route is really costly.</li>
</ul>
<pre><code class="language-python"># Used cached waypoints but start at closest point.
if len(self.waypoints) != 0:
    (index, _) = closest_vertex(
        self.waypoints,
        np.array([self.position[:2]]),
    )

    self.waypoints = self.waypoints[
        index : index + NUM_WAYPOINTS_AHEAD
    ]
    self.target_speeds = self.target_speeds[
        index : index + NUM_WAYPOINTS_AHEAD
    ]
</code></pre>
<ul>
<li>The computation of the gps waypoints happens here:</li>
</ul>
<pre><code class="language-python"># Compute the waypoints
waypoints = self.hd_map.compute_waypoints(
    [
        x,
        y,
        self._goal_location[2],
    ],
    self._goal_location,
</code></pre>
<blockquote>
<p>Sometimes the computation might put the position on the wrong side of the road.
I have put a failsafe mecanism, but that might have other consequences. You can try to revisit this part.
To increase the precision of the map, you can change the parameters in <code>_hd_map.py</code>:</p>
<pre><code class="language-python">       self._grp = GlobalRoutePlanner(
           self._map, 1.0
       )  # Distance between waypoints
</code></pre>
</blockquote>
<h2 id="graph-description"><a class="header" href="#graph-description">Graph Description</a></h2>
<pre><code class="language-yaml">  - id: carla_gps_op
    operator:
      python: ../../carla/carla_gps_op.py
      outputs:
        - gps_waypoints
      inputs:
        opendrive: oasis_agent/opendrive
        objective_waypoints: oasis_agent/objective_waypoints
        position: oasis_agent/position
</code></pre>
<h2 id="graph-viz"><a class="header" href="#graph-viz">Graph Viz</a></h2>
<pre class="mermaid">        flowchart TB
  oasis_agent
subgraph fot_op
  fot_op/op[op]
end
subgraph carla_gps_op
  carla_gps_op/op[op]
end
  oasis_agent -- objective_waypoints --&gt; carla_gps_op/op
  oasis_agent -- opendrive --&gt; carla_gps_op/op
  oasis_agent -- position --&gt; carla_gps_op/op
  carla_gps_op/op -- gps_waypoints --&gt; fot_op/op
</pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="yolov5-operator"><a class="header" href="#yolov5-operator">Yolov5 operator</a></h1>
<p><code>Yolov5</code> object detection operator generates bounding boxes on images where it detects object. </p>
<p>More info here: <a href="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</a></p>
<p><code>Yolov5</code> has not been finetuned on the simulation and is directly importing weight from Pytorch Hub.</p>
<p>In case you want to run <code>yolov5</code> without internet you can clone <a href="https://github.com/ultralytics/yolov5">https://github.com/ultralytics/yolov5</a> and download the weights you want to use from <a href="https://github.com/ultralytics/yolov5/releases/tag/v7.0">the release page</a> and then specify within the yaml graph the two environments variables:</p>
<ul>
<li><code>YOLOV5_PATH: YOUR/PATH</code> </li>
<li><code>YOLOV5_WEIGHT_PATH: YOUR/WEIGHT/PATH</code></li>
</ul>
<p>You can also choose to allocate the model in GPU using the environment variable:</p>
<ul>
<li><code>PYTORCH_DEVICE: cuda # or cpu</code></li>
</ul>
<p>The image size must be specified in order to work. By default it is 1920x1080xBGR.</p>
<h2 id="inputs"><a class="header" href="#inputs">Inputs</a></h2>
<ul>
<li>image as1920x1080xBGR array.</li>
</ul>
<h2 id="outputs"><a class="header" href="#outputs">Outputs</a></h2>
<ul>
<li>Bounding box coordinates as well as the confidence and class label as output.</li>
</ul>
<h2 id="graph-description-1"><a class="header" href="#graph-description-1">Graph Description</a></h2>
<pre><code class="language-yaml">  - id: yolov5
    operator: 
      outputs:
        - bbox
      inputs:
        image: oasis_agent/image
      python: ../../operators/yolov5_op.py
    env:
      PYTORCH_DEVICE: cuda
      YOLOV5_PATH:       /home/dora/workspace/simulate/team_code/dependencies/yolov5 
      YOLOV5_WEIGHT_PATH:  /home/dora/workspace/simulate/team_code/dependencies/yolov5/yolov5n.pt
</code></pre>
<h2 id="graph-viz-1"><a class="header" href="#graph-viz-1">Graph Viz</a></h2>
<pre class="mermaid">        flowchart TB
  oasis_agent
subgraph yolov5
  yolov5/op[op]
end
subgraph obstacle_location_op
  obstacle_location_op/op[op]
end
  oasis_agent -- image --&gt; yolov5/op
  yolov5/op -- bbox as obstacles_bbox --&gt; obstacle_location_op/op
</pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="obstacle-location-operator"><a class="header" href="#obstacle-location-operator">Obstacle location operator</a></h1>
<p>The obstacle location operator match bounding box with depth frame to find an approximative position of obstacles.</p>
<p>There is two logic within it:</p>
<ul>
<li>One is for the ground dot for lane detection.</li>
<li>One is for bounding box obstacle localisation.</li>
</ul>
<p>Both logic are based on he computation of the projection in 2D space of the lidar 3D point and then reusing the index to get the 3D position.</p>
<ul>
<li>In the case of ground dot detection, the approximation is based on a knnr, as we might not have enough data on the floor.</li>
<li>In the case of bounding box, we use first quantile closest point within the bounding box to estimate the distance. We use the first quantile closest point to remove the noise.</li>
</ul>
<p>The mecanism to project the lidar point cloud into a 2D is also used in the <code>plot.py</code> operator. You can use the input <code>lidar_pc</code> within it to help you debug.</p>
<h2 id="inputs-1"><a class="header" href="#inputs-1">Inputs</a></h2>
<ul>
<li>2D Obstacles bounding box.</li>
</ul>
<h2 id="outputs-1"><a class="header" href="#outputs-1">Outputs</a></h2>
<ul>
<li>3D position of obstacles as dot.</li>
</ul>
<h2 id="graph-description-2"><a class="header" href="#graph-description-2">Graph Description</a></h2>
<pre><code class="language-yaml">  - id: obstacle_location_op
    operator: 
      outputs:
        - obstacles
      inputs:
        lidar_pc: oasis_agent/lidar_pc
        obstacles_bbox: yolov5/bbox
        position: oasis_agent/position
      python: ../../operators/obstacle_location_op.py
</code></pre>
<h2 id="graph-viz-2"><a class="header" href="#graph-viz-2">Graph Viz</a></h2>
<pre class="mermaid">        flowchart TB
  oasis_agent
subgraph yolov5
  yolov5/op[op]
end
subgraph fot_op
  fot_op/op[op]
end
subgraph obstacle_location_op
  obstacle_location_op/op[op]
end
  oasis_agent -- lidar_pc --&gt; obstacle_location_op/op
  yolov5/op -- bbox as obstacles_bbox --&gt; obstacle_location_op/op
  oasis_agent -- position --&gt; obstacle_location_op/op
  obstacle_location_op/op -- obstacles --&gt; fot_op/op
</pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="depth-estimation-example"><a class="header" href="#depth-estimation-example">Depth estimation example</a></h1>
<p>MiDaS models for computing relative depth from a single image.</p>
<blockquote>
<p>MiDaS computes relative inverse depth from a single image. The repository provides multiple models that cover different use cases ranging from a small, high-speed model to a very large model that provide the highest accuracy. The models have been trained on 10 distinct datasets using multi-objective optimization to ensure high quality on a wide range of inputs.
Let's add all <code>dora-drives</code> operators that works on image frame, which are:</p>
</blockquote>
<ul>
<li><code>webcam</code> plug in a camera.</li>
<li><code>midas_op</code> computing relative depth from a single image.</li>
<li><code>plot</code> take almost all output from the graph and plot it on the camera frame.</li>
</ul>
<p>the graph will look as follows:</p>
<pre><code class="language-yaml"># graphs/tutorials/webcam_midas_frame.yaml

nodes:
  - id: webcam
    operator:
      python: ../../operators/webcam_op.py
      inputs:
        tick: dora/timer/millis/100
      outputs:
        - image
    env:
      DEVICE_INDEX: 0

  - id: midas_op
    operator:
      outputs:
        - depth_frame
      inputs:
        image: webcam/image
      python: ../../operators/midas_op.py
    env:
      PYTORCH_DEVICE: &quot;cuda&quot;
      MIDAS_PATH: $DORA_DEP_HOME/dependencies/MiDaS/
      MIDAS_WEIGHT_PATH: $DORA_DEP_HOME/dependencies/MiDaS/weights/midas_v21_small_256.pt
      MODEL_TYPE: &quot;MiDaS_small&quot;
      MODEL_NAME: &quot;MiDaS_small&quot;

  - id: plot
    operator:
      python: ../../operators/plot.py
      inputs:
        image: midas_op/depth_frame
</code></pre>
<h3 id="midas-source-code"><a class="header" href="#midas-source-code">MiDaS source code:</a></h3>
<p>You need to link the network to load the algorithm model.
If you are in mainland China, you may need a network proxy to speed up the download.</p>
<p>optional:
Of course, you can also download it in advance and place it under the specified directory. The operation steps are as follows:</p>
<pre><code class="language-bash">cd $DORA_DEP_HOME/dependencies/
git clone git@github.com:isl-org/MiDaS.git
cd MiDaS/weights
# If you don't want to add manual download, the program will also automatically download the model file
wget https://github.com/isl-org/MiDaS/releases/download/v2_1/midas_v21_small_256.pt
cp midas_v21_small_256.pt $HOME/.cache/torch/hub/checkpoints/
</code></pre>
<p>At the same time, open the following comments in the dataflow configuration file # graphs/tutorials/webcam_midas_frame.yaml</p>
<pre><code class="language-yaml">      MIDAS_PATH: $DORA_DEP_HOME/dependencies/MiDaS/
      MIDAS_WEIGHT_PATH: $DORA_DEP_HOME/dependencies/MiDaS/weights/midas_v21_small_256.pt
      MODEL_TYPE: &quot;MiDaS_small&quot;
      MODEL_NAME: &quot;MiDaS_small&quot;
</code></pre>
<ul>
<li>model_type = &quot;DPT_Large&quot;     # MiDaS v3 - Large     (highest accuracy, slowest inference speed)</li>
<li>model_type = &quot;DPT_Hybrid&quot;   # MiDaS v3 - Hybrid    (medium accuracy, medium inference speed)</li>
<li>model_type = &quot;MiDaS_small&quot;  # MiDaS v2.1 - Small   (lowest accuracy, highest inference speed)</li>
</ul>
<h3 id="custom-configuration"><a class="header" href="#custom-configuration">Custom configuration</a></h3>
<ol>
<li>
<p>Pick one or more models, You can <a href="https://github.com/isl-org/MiDaS/#setup">find out more here</a>.</p>
</li>
<li>
<p>Other descriptive information
In this case, we have connected the <code>webcam/image</code> output to the <code>image</code> input of midas_op. <code>midas_op/depth_frame</code> is then connected to the <code>plot/image</code>.
Inputs are prefixed by the node name to be able to separate name conflicts.</p>
</li>
</ol>
<h3 id="to-run"><a class="header" href="#to-run">To run:</a></h3>
<pre><code class="language-bash">dora up
dora start graphs/tutorials/webcam_midas_frame.yaml --attach --hot-reload --name dpt_midas
</code></pre>
<p>Display as follows:</p>
<p align="center">
    <img src="./midas_dpt.png" width="500">
</p>
<blockquote>
<p>I'm currently having issue running all nodes behind the GFW. You can look into it for inspiration.</p>
</blockquote>
<p>Nice 🥳 As you can see, the value of <code>dora</code> comes from the idea that you can compose different algorithm really quickly.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fot-operator"><a class="header" href="#fot-operator">FOT operator</a></h1>
<p>The Frenet Optimal Planner Operator is based on <a href="https://github.com/erdos-project/frenet_optimal_trajectory_planner/">https://github.com/erdos-project/frenet_optimal_trajectory_planner/</a> and wrap the different elements <code>obstacles</code>, <code>position</code>, <code>speed</code> ... into a frenet consumable format. </p>
<p>FOT inputs are:</p>
<pre><code class="language-python">initial_conditions = {
    &quot;ps&quot;: 0,
    &quot;target_speed&quot;: # The target speed
    &quot;pos&quot;: # The x, y current position
    &quot;vel&quot;: # The vx, vy current speed
    &quot;wp&quot;: # [[x, y], ... n_waypoints ] desired waypoints
    &quot;obs&quot;: # [[min_x, min_y, max_x, max_y], ... ] obstacles on the way
}
</code></pre>
<p>There is also a set of hyperparameters that are described below.</p>
<p>As our obstacles are defined as 3D dot we need to transform those dot into <code>[min_x, min_y, max_x, max_y]</code> format. We do that within the <code>get_obstacle_list</code> function. This approximation is very basic and probably need to be revisited.</p>
<p>The output is either a successful trajectory that we can feed into PID. Or it is a failure in which case we send the current position as waypoint.</p>
<h2 id="graph-description-3"><a class="header" href="#graph-description-3">Graph Description</a></h2>
<pre><code class="language-yaml">  - id: fot_op
    operator:
      python: ../../operators/fot_op.py
      outputs:
        - waypoints
      inputs:
        position: oasis_agent/position
        speed: oasis_agent/speed
        obstacles: obstacle_location_op/obstacles
        gps_waypoints: carla_gps_op/gps_waypoints
</code></pre>
<h2 id="graph-viz-3"><a class="header" href="#graph-viz-3">Graph Viz</a></h2>
<pre class="mermaid">        flowchart TB
  oasis_agent
subgraph carla_gps_op
  carla_gps_op/op[op]
end
subgraph fot_op
  fot_op/op[op]
end
subgraph obstacle_location_op
  obstacle_location_op/op[op]
end
subgraph pid_control_op
  pid_control_op/op[op]
end
  carla_gps_op/op -- gps_waypoints --&gt; fot_op/op
  obstacle_location_op/op -- obstacles --&gt; fot_op/op
  oasis_agent -- position --&gt; fot_op/op
  oasis_agent -- speed --&gt; fot_op/op
  fot_op/op -- waypoints --&gt; pid_control_op/op
</pre>
<h2 id="hyperparameter-descriptions"><a class="header" href="#hyperparameter-descriptions">Hyperparameter descriptions</a></h2>
<pre><code class="language-bash">        initial_conditions (dict): dict containing the following items
            ps (float): previous longitudinal position
            target_speed (float): target speed [m/s]
            pos (np.ndarray([float, float])): initial position in global coord
            vel (np.ndarray([float, float])): initial velocity [m/s]
            wp (np.ndarray([float, float])): list of global waypoints
            obs (np.ndarray([float, float, float, float])): list of obstacles
                as: [lower left x, lower left y, upper right x, upper right y]
        hyperparameters (dict): a dict of optional hyperparameters
            max_speed (float): maximum speed [m/s]
            max_accel (float): maximum acceleration [m/s^2]
            max_curvature (float): maximum curvature [1/m]
            max_road_width_l (float): maximum road width to the left [m]
            max_road_width_r (float): maximum road width to the right [m]
            d_road_w (float): road width sampling discretization [m]
            dt (float): time sampling discretization [s]
            maxt (float): max prediction horizon [s]
            mint (float): min prediction horizon [s]
            d_t_s (float): target speed sampling discretization [m/s]
            n_s_sample (float): sampling number of target speed
            obstacle_clearance (float): obstacle radius [m]
            kd (float): positional deviation cost
            kv (float): velocity cost
            ka (float): acceleration cost
            kj (float): jerk cost
            kt (float): time cost
            ko (float): dist to obstacle cost
            klat (float): lateral cost
            klon (float): longitudinal cost
    Returns:
        result_x (np.ndarray(float)): x positions of fot, if it exists
        result_y (np.ndarray(float)): y positions of fot, if it exists
        speeds (np.ndarray(float)): speeds of fot, if it exists
        ix (np.ndarray(float)): spline x of fot, if it exists
        iy (np.ndarray(float)): spline y of fot, if it exists
        iyaw (np.ndarray(float)): spline yaws of fot, if it exists
        d (np.ndarray(float)): lateral offset of fot, if it exists
        s (np.ndarray(float)): longitudinal offset of fot, if it exists
        speeds_x (np.ndarray(float)): x speeds of fot, if it exists
        speeds_y (np.ndarray(float)): y speeds of fot, if it exists
        params (dict): next frenet coordinates, if they exist
        costs (dict): costs of best frenet path, if it exists
        success (bool): whether a fot was found or not

</code></pre>
<p>Ref: https://github.com/erdos-project/frenet_optimal_trajectory_planner/blob/master/FrenetOptimalTrajectory/fot_wrapper.py</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pid-control-operator"><a class="header" href="#pid-control-operator">PID Control operator</a></h1>
<p><code>pid</code> control operator computes the command that needs to be executed to follow the given waypoints. 
It reacts to the car current speed and position in a way that accelerates or brake according to previous inputs.</p>
<h2 id="inputs-2"><a class="header" href="#inputs-2">Inputs</a></h2>
<ul>
<li>waypoints coordinates to follow.</li>
</ul>
<h2 id="outputs-2"><a class="header" href="#outputs-2">Outputs</a></h2>
<ul>
<li>throttle, steering (rad) and braking.</li>
</ul>
<h2 id="graph-description-4"><a class="header" href="#graph-description-4">Graph Description</a></h2>
<pre><code class="language-yaml">  - id: pid_control_op
    operator:
      python: ../../operators/pid_control_op.py
      outputs:
        - control
      inputs:
        position: oasis_agent/position
        speed: oasis_agent/speed
        waypoints: fot_op/waypoints
</code></pre>
<h2 id="graph-viz-4"><a class="header" href="#graph-viz-4">Graph Viz</a></h2>
<pre class="mermaid">        flowchart TB
  oasis_agent
subgraph fot_op
  fot_op/op[op]
end
subgraph pid_control_op
  pid_control_op/op[op]
end
  oasis_agent -- position --&gt; pid_control_op/op
  oasis_agent -- speed --&gt; pid_control_op/op
  fot_op/op -- waypoints --&gt; pid_control_op/op
  pid_control_op/op -- control --&gt; oasis_agent
</pre>
<h2 id="hyperparameters-consider-changing"><a class="header" href="#hyperparameters-consider-changing">Hyperparameters consider changing</a></h2>
<p>See: https://en.wikipedia.org/wiki/PID_controller</p>
<pre><code>pid_p = 0.1
pid_d = 0.0
pid_i = 0.05
dt = 1.0 / 20   
</code></pre>
<p>Example reaction:</p>
<p align="center">
<img src="https://upload.wikimedia.org/wikipedia/commons/c/c0/Change_with_Ki.png" width="800">
</p><div style="break-before: page; page-break-before: always;"></div><h1 id="overview"><a class="header" href="#overview">Overview</a></h1>
<pre class="mermaid">        flowchart TB
  oasis_agent
subgraph carla_gps_op
  carla_gps_op/op[op]
end
subgraph yolov5
  yolov5/op[op]
end
subgraph obstacle_location_op
  obstacle_location_op/op[op]
end
subgraph fot_op
  fot_op/op[op]
end
subgraph pid_control_op
  pid_control_op/op[op]
end
subgraph ___dora___ [dora]
  subgraph ___timer_timer___ [timer]
    dora/timer/secs/1[\secs/1/]
  end
end
  pid_control_op/op -- control --&gt; oasis_agent
  dora/timer/secs/1 -- tick --&gt; oasis_agent
  oasis_agent -- objective_waypoints --&gt; carla_gps_op/op
  oasis_agent -- opendrive --&gt; carla_gps_op/op
  oasis_agent -- position --&gt; carla_gps_op/op
  oasis_agent -- image --&gt; yolov5/op
  oasis_agent -- lidar_pc --&gt; obstacle_location_op/op
  yolov5/op -- bbox as obstacles_bbox --&gt; obstacle_location_op/op
  oasis_agent -- position --&gt; obstacle_location_op/op
  carla_gps_op/op -- gps_waypoints --&gt; fot_op/op
  obstacle_location_op/op -- obstacles --&gt; fot_op/op
  oasis_agent -- position --&gt; fot_op/op
  oasis_agent -- speed --&gt; fot_op/op
  oasis_agent -- position --&gt; pid_control_op/op
  oasis_agent -- speed --&gt; pid_control_op/op
  fot_op/op -- waypoints --&gt; pid_control_op/op
</pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-format"><a class="header" href="#data-format">Data format</a></h1>
<p>All messages should be byte messages.</p>
<p>Best practice is to use C-order numpy arrays bytes.</p>
<p>they can be generated via the <code>.tobytes()</code> method from numpy arrays.</p>
<p>They can be read via <code>np.frombuffer</code>.</p>
<h2 id="currently-used-message-format"><a class="header" href="#currently-used-message-format">Currently used message format</a></h2>
<pre><code class="language-python">## position of the car (1, 7) 
### qx, qy, qz, qw are angles quaternion.
position = np.array([x, y, z, qx, qy, qz, qw])

## frames (HEIGHT, WIDTH, 4)
frame = np.array([[[b, g, r, i], ... n_width ... ], ... n_height ... ])

## Obstacles without location (-1, 6)
bbox_2d = np.array([[min_x, max_x, min_y, max_y, confidence, label], ... n_bbox ... ])

## Obstacles with locations (-1, 5)
obstacles = np.array([[x, y, z, confidence, label], ... n_obstacle ... ])

## waypoints to follow. Shape (-1, 3)
waypoints = np.array([[x, y, speed], ... n_waypoint ... ])

## control for the car (1, 3)
control = np.array([throttle, steer, brake])
</code></pre>

                        <script src="https://giscus.app/client.js"
                                data-repo="dora-rs/dora-drives"
                                data-repo-id="R_kgDOHPuVyw"
                                data-category="General"
                                data-category-id="DIC_kwDOHPuVy84CRC_Z"
                                data-mapping="pathname"
                                data-strict="0"
                                data-reactions-enabled="1"
                                data-emit-metadata="0"
                                data-input-position="top"
                                data-theme="preferred_color_scheme"
                                data-lang="en"
                                data-loading="lazy"
                                crossorigin="anonymous"
                                async>
                        </script>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                        
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                            </nav>

        </div>

        
        
        
                <script>
            window.playground_copyable = true;
        </script>
        
        
                <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>
        
        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
                <script src="mermaid.min.js"></script>
                <script src="mermaid-init.js"></script>
        
                        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
                
    </div>
    </body>
</html>
